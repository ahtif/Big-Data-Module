Jacobian Code Generated by Source
Transformation and Vertex Elimination can be as

Efﬁcient as Hand—Coding

SHAUN A. FORTH, MOHAMED TADJOUDDINE and JOHN D. PRYCE
Cranfield University (Shrivenham Campus)

and

JOHN K. REID

JKR Associates

 

This paper presents the ﬁrst extended set of results from ELIAD, a source—transformation imple—
mentation of the vertex—elimination Automatic Differentiation approach to calculating the Jaco-
bians of functions deﬁned by Fortran code (Griewank and Reese, Automatic Differentiation of
Algorithms: Theory, Implementation, and Application, 1991, pp: 126—135): We introduce the
necessary theory in terms of well known algorithms of numerical linear algebra applied to the
linear, extended Jacobian system that prescribes the relationship between the derivatives of all
variables in the function code: Using an example, we highlight the potential for numerical insta—
bility in vertex—elimination: We describe the source transformation implementation of our tool
ELIAD and present results from 5 test cases, 4 of which are taken from the MINPACK—2 collection
(Averick et al, Report ANL/MCS—TM—150, 1992) and for which hand—coded Jacobian codes are
available: On 5 computer/compiler platforms, we show that the Jacobian code obtained by ELIAD
is as efﬁcient as hand—coded Jacobian code: It is also between 2 to 20 times more efﬁcient than
that produced by current, state of the art, Automatic Differentiation tools even when such tools
make use of sophisticated techniques such as sparse Jacobian compression: We demonstrate the
effectiveness of reverse—ordered pre—elimination from the (successively updated) extended Jacobian
system of all intermediate variables used once: Thereafter, the monotonic forward/reverse ordered
eliminations of all other intermediates is shown to be very efﬁcient: On only one test case were
orderings determined by the Markowitz or related VLR heuristics found superior: A re—ordering
of the statements of the Jacobian code, with the aim of reducing reads and writes of data from
cache to registers, was found to have mixed effects but could be very beneﬁcial:

Categories and Subject Descriptors: G,1,0 [General]: Stability (and instability); G,1,4 [Quadra-
ture and Numerical Differentiation]: Automatic differentiation; G,1,5 [Roots of Nonlinear
Equations]: Systems of equations; G,1,6 [Optimization]: Least squares methods; G,4 [Math-
ematical Software]: Efﬁciency

General Terms: Algorithms, Performance
Additional Key Words and Phrases: Jacobians, source transformation, vertex elimination

 

 

©ACM, (2004): This is the author’s version of the work: It is posted here by permission
of ACM for your personal use: Not for redistribution: The deﬁnitive version was published
in ACM Transactions on Mathematical Software , VOL 30, ISSN:0098—3500, (September 2004)
http://doi,acm,org/10,1145/1024074,1024076 Work funded by the UK’s EPSRC and MOD under
grant GR/R21882: Author’s addresses: S:A: Forth, M: Tadjouddine (Engineering Systems De—
partment) & J:D: Pryce (Communications and Information Sytems Engineering), Cranﬁeld Uni—
versity (Shrivenham Campus), Shrivenham, Swindon SN6 8LA, UK; J:K: Reid, JKR Associates
and Rutherford Appleton Laboratory, Atlas Centre, Rutherford Appleton Laboratory, Didcot,
Oxon OX11 OQX, UK,

Permission to make digital/hard copy of all or part of this material without fee for personal
or classroom use provided that the copies are not made or distributed for proﬁt or commercial
advantage, the ACM copyright/server notice, the title of the publication, and its date appear, and
notice is given that copying is by permission of the ACM, Inc: To copy otherwise, to republish,
to post on servers, or to redistribute to lists requires prior speciﬁc permission and/or a fee:

© 2004 ACM 0098—3500/2004/1200—0266 $5.00
ACM Transactions on Mathematical Software, Vol. 30, No. 3, Sep. 2004, Pages 2667299.

October 11, 2004

GLOSSARY
A Adjoint matrix
at]: Local derivative Bin/Bu]:

C Matrix of local derivatives

D Solution of extended Jacobian system
e7; i—th column of unit matrix

f Given function

Vf(x) Jacobian matrix, of order m X n

m Number of dependent variables

n Number of independent variables

N = n + p + m

N]C]:1Number of unit entries Cid» : il in C
N]C]¢1Number of non—unit entries in C
Nevals Number of separate evaluations timed
Nmpet Number of repeats of timing test

Jacobian Code by Vertex Elimination ' 267

p Number of intermediate variables

P Matrix associated with extended system
q Number of columns in S

Q Matrix associated with extended system
S Seed matrix of order n X q or m X q

5,; i—th column of S

u, i—th code variable

11 All active variables

v Variables of the code—list

w, i—th intermediate variable

W Intermediate variables

W Computational work

x Independent variables

y Dependent variables

1. INTRODUCTION

Automatic Differentiation (AD) concerns the process of taking a function y : f(x),
with f deﬁned by a computer code, that maps independent variables x 6 IE” to
dependent variables y 6 [RM and then constructing new code that will also cal—
culate derivatives of f. AD relies on the fact that each statement of the code
involving ﬂoating—point numbers may be individually differentiated. The forward
mode of AD creates new code that, for each of the codes variables, calculates the
numerical values of the variable and its derivatives with respect to the indepen—
dent variables. Reverse (or adjoint) mode AD produces code that passes forward
through the original code storing information required for a reverse pass in which
the sensitivities of the nominated dependent variables to changes in the values of
the codes variables are calculated. A thorough introduction to the ﬁeld may be
found in [Griewank 2000] and further theoretical results and applications may be
found in the collections [Griewank and Corliss 1991, Berz et al. 1996, Corliss et al.
2001]

AD software tools exist for codes written in Fortran [Bischof et al. 1996, Gier—
ing and Kaminski 1998, Faure and Papegay 1998, Pryce and Reid 1998], C and
C++ [Bischof et al. 1997, Bendtsen and Stauning 1996, Griewank et al. 1996] and
Matlab [Verma 1998, Forth 2001, Bischof et al. 2002] amongst other programming
languages. These tools implement AD in one of two ways, source transformation
or operator overloading.

Source transformation [Griewank 2000, Section 5.7—5.8] involves use of sophis—
ticated compiler techniques. For the forward mode, new code is produced that,
when executed, calculates derivatives as well as values for the dependent variables.
In reverse mode, new code is produced that will calculate so—called adjoint values
backwards through an enhanced version of the original code in such a way as to
calculate STVf(x) for any matrix S with in rows.

Alternatively, the operator overloading approach [Griewank 2000, Section 5.1—5.6]
utilises a feature available in some modern, object—oriented computer languages.
New types of variable are deﬁned that store the variables value and, for forward
mode, also store its derivatives. For reverse mode, instead of derivatives, sufﬁcient
information is saved (to a so—called tape) to enable the required reverse propagation
of sensitivities. In both cases, arithmetic operations and intrinsic functions are

ACM Transactions on Mathematical Software, Vol. 30, No. 3, Sep. 2004.

268 ' Shaun A. Forth et al. March 8th 2004

extended to the new types.

For languages such as Fortran or C (for which optimising compilers exist) and
applied to code featuring scalar assignments, it is generally found that the source
transformation approach produces more efﬁcient derivative code (see for exam—
ple [Tadjouddine et al. 2001: Pryce and Reid 1998]).

Our work is motivated by the particular requirement for J acobian code in solving
systems of nonlinear equations via Newton’s, or a related method. Such systems
arise in applications such as computational ﬂuid dynamics, computational chem—
istry, and data—ﬁtting. For examples, see the test cases of [Averick et al. 1992] and
references therein. In such cases, the system J acobian may be needed for direct solu—
tion of a Newton update. Alternatively, when using Krylov—based solvers for which
Jacobian—vector products may be evaluated by ﬁnite differencing or conventional
AD tools, the J acobian frequently needs to be determined for preconditioning, for
example, when using incomplete LU factorisation [Hovland and McInnes 2001].

When discussing sparsity, we will use the term entry for a matrix coefﬁcient that
we represent explicitly because we cannot be sure that it is zero. An entry may
‘accidentally’ have the value zero, so the term ‘nonzero’ is not suitable.

The structure of the rest of this paper is as follows. In Section 2 and Section 3,
we review the matrix interpretation of the conventional forward and reverse modes
of AD and then the vertex elimination approach of [Griewank and Reese 1991].
This is necessary to understand the implementation of our vertex elimination tool
ELIAD. ELIAD is implemented via source transformation as described in Section 4.
Our test environment is explained in Section 5. In Section 6, we present and
discuss an extended set of results from ELIAD. For these test cases, we demonstrate
that AD via vertex elimination and source transformation enables the calculation
of Jacobians as fast as hand—coded Jacobian code and with more than twice the
efﬁciency of present AD tools and techniques. Section 7 presents conclusions and
the outlook for extending ELIAD’s coverage of Fortran and improving it to produce
even faster Jacobian code.

2. MATRIX INTERPRETATION OF AUTOMATIC DIFFERENTIATION

The matrix interpretation of AD [Griewank 2000, Section 8.1] allows us to View
the standard forward and reverse modes of AD in terms of the well—known forward
and back substitution algorithms for systems of linear equations with triangular
coefﬁcient matrices. In Section 2.1, we introduce the lower triangular extended
Jacobian system and in Section 2.2 we indicate how this system is solved in forward
and reverse mode AD. We then consider the exploitation of sparsity, the number of
ﬂoating—point operations involved, and how to treat the temporary variables that
occur within statements.

2.1 The Extended Jacobian System
For a function f : x 6 1R” i—> y E [Rm that is deﬁned by computer code and which

we wish to differentiate, we deﬁne three (sub)groups of the codes variables:

independent variables: x : (xhi : 1, . . . ,n) whose values must be supplied and
with respect to which the derivatives 8y/8x are required,
dependent variables: y : (yhi : 1,. . . , m) which must be calculated and whose

ACM Transactions on Mathematical Software, Vol. 30, No. 3, Sep. 2004.

October 11, 2004 Jacobian Code by Vertex Elimination ' 269

derivatives are required,

intermediate variables: w : (whi : 1, . . . ,p) whose values are calculated (perhaps
indirectly) from the x and which are needed to calculate
(perhaps indirectly) the y.

Collectively, these variables are termed active variables. We deﬁne inactive variables
as those which are not active.

EXAMPLE 2.1 EXAMPLE CODE. For the code fragment

wl :log(x1*x2)
1.02 :$2*$§*a
1.03 :b*w1+x2/x3
1/1 =wf+w2i$2

1/2 : x/wa * w2

we wish to calculate 8(y1,y2)/8(x1,x2,x3). Hence x1,x2,$3 are the n:? indepen—
dent variables, y1,y2 are the m:2 dependent variables, and w1,w2,w3 are the p:?
intermediate variables. Values of the variables a,b must be supplied but, since we
do not require derivatives with respect to them, they are inactive.

For the purposes of analysis, we regard an execution of the function code as
deﬁning N : n +p + m internal variables u,, i : 1, . . . ,N in the following manner.
First there are n copies of the independent variables to the internal variables,

ui:x,-, i:1,...n, (1)
followed by the p + m statements of the code
ui:<I>,-({uj}j<,-), i:n+1,...,N, (2)

where the precedence relation j < i means that the variable uj is involved in the
expression (1),. Each (1),- represents a composition of one or more elemental/intrinsic
functions or elemental/intrinsic operators of the programming language. For the
most part, we will assume that no expression for a dependent variable involves
another dependent variable, that is, ifi > n +p and j < i, then j S n +p. This
may be ensured by making a copy of any dependent variable used to calculate
another dependent variable, but we will also explain how to avoid the need for this.

EXAMPLE 2.2 EXAMPLE CODE. The code fragment ofErample 2.1 may be rewrit—
ten in the form of (I) to (2) as

”1 :$1

”2 :$2

”3 :$3

M4 :<I>4(u1,u2) :log(u1*u2)
u5 :<I>5(u2,u3) :u2*u§7a
M6 :<I>6(u2,u3,u4) :b*u4+u2/u3
M7 : (1)7(u2,u4,u5) :ui+u5 7712

He : @8015, H6) : x/ue * H5

ACM Transactions on Mathematical Software, Vol. 30, No. 3, Sep. 2004.

270 ' Shaun A. Forth et al. March 8th 2004

The assignments of (1) and (2) can be written as the following system of nonlinear
equations

0:x,-*u,-, i:1,...,n
0: ¢i({uj}j<i)iui)7 ’t:n+I,...,N
We assume that the functions (I), have continuous ﬁrst derivatives, deﬁne the gra—

dient operator V by V : (8/8x1, . . . ,8/8xn), and differentiate (3) with respect to
the independent variables x1, . . . , x”, to give

’VW :iei, i=1,...,n
ZMCMVWPVM =0, i:n+1,...,N 7

(3)

(4)

where e,- is the n—vector with unit entry in position i and cm- are the local derivatives
CM : [My/8w. On deﬁning the matrix C : {Ct,j}1gt,jgN to be composed of all
such entries and zeros elsewhere, the linear system (4) can be compactly rewritten
as the ertended Jacobian system

(c a IN)D = 7P, (5)

with D : Wu and
I
p : ] .. ] . (6
0(m.+p)><n )

The matrix C*IN is called the ertended Jacobian and is necessarily lower triangular
because each value u,- is calculated from previously calculated values uj with j < i.
We deﬁne N]C]:1 to be the number of entries in C taking the value i1 and NM?“
to be the number of other entries.

EXAMPLE 2.3 EXTENDED JACOBIAN SYSTEM. For our erample code, the em—
tended Jacobian system is given by

 

 

 

 

 

*1 Val _ *1 0 0
*1 VH2 0 *1 0
*1 Vug 0 0 *1
C471 C472 *1 VH4 _ 0 0 0
C572 C573 *1 VH5 i 0 0 0 7
C672 C673 C674 *1 VH6 0 0 0
C772 C774 C775 *1 VH7 0 0 0
C8,5 68,6 *1 VH8 _ 0 0 0
with entries in the ertended Jacobian given by
04,1 :1/7117 C6,4 : b
04,2 :1/7127 C7,2 : ’17
C5,2 : Hg, 07,4 : 2 * 7147 (7)
C573 :2*H2*H3, C775 :1, .
C6,2 :1/7137 C8,5 : *1
C6,3 : ’712/7137 C8,6 :1/(2\/u—6)

We see that N]C]:1 : 3 and N]c]¢1 : 9-

ACM Transactions on Mathematical Software, Vol. 30, No. 3, Sep. 2004.

October 11, 2004 Jacobian Code by Vertex Elimination ' 271

2.2 Forward and Reverse Mode AD

In the forward method, we solve the lower triangular system (5) for the n columns
of D by forward substitution, then extract the last m rows to obtain the Jacobian.
By deﬁning the matrix Q to be

Q : |:0(n+p)><m.:| 7 (8)

m

we can express the solution of equation (5) and the extraction of the ﬁnal m rows
of D as

Vf = QT(C * IN)’1(7P). (9)
If we group the terms as Vf : QT[(C * IN)’1(*P)], we have a formal de—
scription of forward mode AD. Alternatively, by grouping equation (9) as Vf :
](*QT)(C * IN)’1] P, and deﬁning adjoint quantities A as the solution, by back—
substitution, of the upper—triangular system,

(C * IN)TA : *Q, (10)

we obtain the Jacobian as the ﬁrst n columns of AT, or more formally Vf : ATP.
This is a matrix interpretation of reverse mode AD [Griewank 2000, p. 161*162].
It should be noted that present AD tools, such as TAMC, are designed to calcu—
late an arbitrary J acobian—matrix product (forward mode) or an arbitrary matrix—
Jacobian product (reverse mode). Consequently, they allow for P to be of the form

P : ] S ] for an arbitrary full matrix S with n rows and q columns and Q
0(7n+p)Xq

to be of the form Q : ] DWTSplxq ] for an arbitrary full matrix S with m rows and

q columns. Such a matrix S is termed a seed matrir. We denote its i—th column
by 5,. Of course, the solutions D of (5) or A of (10) are then no longer simple
derivatives or adjoints, but are linear combinations Vf s,- of derivatives or STVf of
adjoints. It may readily be veriﬁed that if S is full, the solution of (5) or (10) is full
(apart from ‘accidental’ cancellations where two values sum to zero) since each of
the last p+m rows and each of the ﬁrst n+p columns of C has at least one entry.
To calculate the Jacobian Vf using such tools, the seed matrix S must be set to In
(forward mode) or Im (reverse mode).
Let us consider these two approaches for our Example 2.3.

EXAMPLE 2.4 FORWARD MODE AD. Forward mode AD solves the linear system
ofErample 2.3 using9x3 : 27 multiplications and 7x3 : 21 additions/subtractions,
that is, 48 floating—point operations (flops).

EXAMPLE 2.5 REVERSE/ADJOINT MODE AD. For Emample 2.3 the adjoint sys—

tem is solved using 9 x 2 : 18 multiplications and 5 x 2 : 10 additions/subtractions,
that is, 28 flops.

For our simple example, reverse mode AD uses fewer arithmetic operations than
forward mode. In both examples, we have neglected multiplications by unit entries
CM : i1. We have, however, counted the multiplications and additions by zero
that occur when S is not full. This is because the basic operation is the addition
of a multiple of Va, to Vuj and testing for zeros in Va, would slow the code.

ACM Transactions on Mathematical Software, Vol. 30, No. 3, Sep. 2004.

272 ' Shaun A. Forth et al. March 8th 2004

2.3 Taking Account of Sparsity

If no account is taken of sparsity in S and if the given function f takes time W(f),
forward mode AD calculates the Jacobian in time O(n) x W(f) and reverse mode
does so in time O(m) x W(f) [Griewank 2000]. For problems with many independent
variables (n > 1) and few dependents, we see that reverse mode is to be preferred
(e.g. large scale optimization).

For Jacobians with known sparsity pattern, there are established techniques for
reducing the size of the seed matrix S needed to calculate all nonzero entries of
Vf(x) [Griewank 2000, Chap. 7]. Such techniques, collectively termed Jacobian
compression, frequently reduce the size, but not to less than q columns in forward
mode or q rows in reverse with q the maximum number of entries in any row
(forward mode) or column (reverse mode) of the Jacobian.

Another possibility is to employ a data structure that permits all operations with
values that are known to be zero to be avoided completely. This is the approach
taken by our code ELIAD. Explicit code is generated for each of the elimination
operations

Cm : Cm * 019ka

for which Ci,k and Clw' are both entries. No code is generated where is it known a
priori that either Ci,k or Chi is always zero.

The techniques discussed so far in this section assume that the sparsity structure
is ﬁxed, so that the set of vectors or generated code needs to be determined once,
and they are thus static exploitations of the Jacobian sparsity. An alternative is
via the dynamic exploitation of sparsity. Here, the data are stored in some sparse
format that is adjusted dynamically (at run time). Prime examples of such an
approach are the use of the SparsLinC library in ADIFOR [Bischof et al. 1996:
Bischof et al. 1996], use of sparse options in AD01 [Pryce and Reid 1998], and
exploitation of the sparse matrix class of Matlab [Coleman and Verma 1998: Forth
2001]. The formal operations count for such an approach is low, but the overhead
of manipulating the sparse storage typically makes them uncompetitive unless the
sparsity structure is not ﬁxed [Griewank 2000, p. 156]. The approach is also of use
in determining a ﬁxed sparsity structure prior to J acobian compression.

2.4 Computational Cost of Forward and Reverse Mode AD

For a given function f, we assume the computational cost W(Vf) of evaluating its
derivatives via AD may be written as

W(Vf) : W(f) + W(C) + W(linear solve), (11)
where:
* W(f) is the cost of evaluating the original function.

* W(C) is the cost of evaluating C, that is, all the local derivatives cm.

* W(linear solve) is the cost of solving the linear system (5) or (10).

We will measure the cost in ﬂoating—point operations (ﬂops), concentrating on
W(linear solve) since we can have little inﬂuence on the other two costs.

ACM Transactions on Mathematical Software, Vol. 30, No. 3, Sep. 2004.

October 11, 2004 Jacobian Code by Vertex Elimination ' 273

[Griewank 2000, Chap. 3] takes account of the time required for data transfers
from and to the ﬂoating—point registers under rather conservative conditions. How—
ever, as we shall see in Sections 3.1 and 4, our optimized derivative code may access
extended Jacobian entries in a non—sequential way and this, together with the effects
of compiler optimizations make this time difﬁcult to quantify and so, regretfully,
we neglect it in this subsection. This approximation is in line with that of previous
work [Griewank and Reese 1991], [Griewank 2000, Chap. 8].

For forward mode AD and without exploiting J acobian compression or dynamic
sparsity (see Section 2.3), the cost of the linear solve of (5) is given by

W(linear solve (forward mode)) : n(2N]C]¢1 + N]C]:1 * p * m). (12)

We incur n multiplications when multiplying a row by a CM 7é i1 and n additions
for all entries as we accumulate the vectors to the u,. We subtract the cost of
n(m + p) additions since the ﬁrst gradient in each line of the forward substitution
is assigned and not added to the intermediates or dependent’s derivatives.

For a sparse Jacobian, (forward) Jacobian compression techniques frequently
allow the Jacobian Vf(x) to be recovered from q < n Jacobian—vector products
Vf(x)s,-, i : 1, . . . , q. Clearly, the cost of calculating this is given by (12), but with
n replaced by q.

Similarly, for reverse mode AD, the computational cost of the linear solve asso—
ciated with (10) is

W(linear solve (reverse mode)) : m(2N]C]¢1 + N]C]:1 * p * n). (13)

If Jacobian compression is possible, we propagate q vector—Jacobian products stflx),
j : 1, . . . ,q, and extract the Jacobian with a resulting cost for the linear solve given
by (13) with m replaced by q.

2.5 Statement—Level versus Code—List Differentiation

So far in this paper, we have differentiated each statement locally with respect to
the active variables that appear in its right—hand side. This is termed a statement—
level diﬂerentiation. An alternative, used later in this paper, is based on the code
list [Griewank 2000], in which the original program is rewritten so that the right—
hand side of each statement has a single unary or binary operation.

EXAMPLE 2.6 CODE—LIST. A possible code list for Erample 2.1 is:

E 119 :1/113
’Ul :$1
1} : $ U10 : U2 * ’U9
U2:$2 U11:b*’U5
3: 3

_ 1112 : 1111 + 1110
U4 7 U1 * U2 U13 — U8 7 U2 (14)
v5 : log(v4) _ 2

2 ”14 ’95

U6 :U3

1115 : V1112
1116 : 1114 + U13

”17 i ”15 7 U8

U7:’U6*’U2
v8:v7*a

in which the division has been replaced by the nonlinear reciprocal operation followed
by a multiplication, and statements are ordered such that the ﬁnal two variables
1116,1117 correspond to the dependent variables y1,y2.

ACM Transactions on Mathematical Software, Vol. 30, No. 3, Sep. 2004.

274 ' Shaun A. Forth et al. March 8th 2004

3. AUTOMATIC DIFFERENTIATION BY VERTEX ELIMINATION

If there are no intermediate variables (p:0), the second line of equation (4) shows
that the matrix C is the Jacobian Vf(x). [Griewank and Reese 1991] therefore sys—
tematically reduced the extended Jacobian system (5) by Gaussian elimination of
the intermediate variables to obtain the J acobian. Their original analysis actually
used the computational graph but was later reinterpreted [Griewank 2000] using
the extended Jacobian. We defer the graph interpretation until Section 3.2, ﬁrst
introducing vertex elimination via the extended J acobian description in Section 3.1
since we believe it is more accessible to the scientiﬁc computing community. We
discuss the connection to standard AD algorithms in Section 3.3. In Section 3.4, we
discuss techniques to determine the order in which we choose to eliminate interme—
diates from the extended Jacobian. In Section 3.5, we show that a poor choice of
ordering may lead to accumulation of roundoff and instability. Section 3.6 describes
techniques for statement—level differentiation of the function, which motivate the
pre—elimination strategy of Section 3.7.

3.1 Reduction of the Extended Jacobian

To reduce the extended J acobian, we apply Gaussian elimination, pivoting on the
diagonal entries of the columns corresponding to intermediate variables. In each
case, we add multiples of the pivot row to later rows to create zeros below the
diagonal in the pivot column. Since the pivot row and pivot column are no longer
relevant, we then discard them * this is what makes it Gaussian, rather than the
less efﬁcient Jordan, elimination. Note that the form of the extended Jacobian is
preserved in the sense that it remains lower triangular with diagonal entries equal
to *1.

We illustrate this with the extended Jacobian system of Example 2.3. We start
with the system

 

 

 

 

*1 VH1 *1
*1 VH2 *1
*1 VH3 *1
04,1 04,2 *1 VHA _ r
C5,2 05,3 *1 VH5 T i (10)
C6,2 C6,3 C6,4 *1 VH6
*1 C774 I *1 VH7
*1 68,6 *1 VH8

First, pivoting on diagonal 4, we add multiples c674 and C774 of row 4 to rows 6
and 7. This creates two new entries, or ﬁll—ins, c671 : c674 * C471, C771 : C774 * C471,
and modiﬁes two entries, c672 : c672 + c674 * C472, C772 : *1 + C774 * C472. Row and

ACM Transactions on Mathematical Software, Vol. 30, No. 3, Sep. 2004.

October 11, 2004 Jacobian Code by Vertex Elimination ' 275

column 4 are now discarded to give

 

 

 

 

*1 VH1 *1
*1 VH2 *1
*1 VH3 *1
C572 C573 *1 VH5 : . (16)
C6,1 C6,2 C6,3 *1 VH6
C771 C772 I *1 VH7
*1 68,6 *1 VH8

We now pivot on the new diagonal 4. This requires one modiﬁcation C772 :
c7,2 + C572 and three ﬁll—ins C773 : C573, c872 : *C572, c873 : *C53. The pivot row
and column are now discarded.

For the ﬁnal pivot step, we have two modiﬁcations c872 : c872 + c876 * c672, c873 :
cs,3 + C8,6 * C6,3 and one ﬁll—in c871 : c876 * c671 to yield

 

*1 VH1 *1
*1 VH2 *1
*1 VH3 : ’1
C7,1 C7,2 07,3 *1 VH7
C8,1 C8,2 C8,3 *1 Vus

 

Clearly, we now have

Vf(x) : [67,1 67,2 67,3]
C8,1 C8,2 C8,3
and, neglecting the cost of sign changes, the Jacobian has been calculated with a
total cost of 12 ﬂoating—point operations (ﬂops). This is a substantial saving over
the forward mode of Example 2.4 (48 ﬂops) and reverse mode of Example 2.5 (28
ﬂops).

It is trivial to allow for entries CM in the ﬁnal m columns of C (when some
expressions for dependent variables involve other dependent variables). We simply
perform Gaussian elimination on each such column, but do not discard the pivot
row.

3.2 The Computational Graph Description

The description of Section 3.1 suggests that Gaussian elimination, rather than
vertex elimination, would be a more appropriate name for the algorithms of this
paper. To explain the origins of the term vertez elimination we return to the original
graph based description of [Griewank and Reese 1991].

In Figure 1(a), we show the computational graph for our example before any
eliminations have been performed. The graph has vertices labelled 1 to 8 cor—
responding to the internal variables ul to ug of Example 2.2. Vertices 1 to 3,
corresponding to the independent variables, are placed at the bottom of the graph
and the dependents, labelled 7 and 8, are at the top. Vertices 4 to 6, corresponding
to intermediates, lie in the centre. There is a directed edge from vertex j to vertex
i ifj < i and the edge is labelled with the associated local derivative CM as given
in (7). This labelling is what makes it the “linearized” computational graph.

The derivative of any dependent variable i with respect to any independent j is

ACM Transactions on Mathematical Software, Vol. 30, No. 3, Sep. 2004.

276 ' Shaun A. Forth et al. March 8th 2004

       

O 7 O 8
C8,6
6
C8,5
5 .
C6,3
65,3
3
a) Original computational graph b) After elimination of vertex 4
O 7 O 8 O 7 O 8

 

c) After elimination of vertex 5 d) After elimination of vertex 6
Fig: 1: Graph representation of the vertex elimination of Section 31

given by the sum of the products of all edge labels for paths connecting j to i in
the graph [Griewank 2000, p. 169]. For example, u7 is connected to u2 via three
paths given by the 3 sets of edge labels {6774,0472}, {C772} and {C775, C572} and so,

(9H7

8H2
Consequently, we see that a vertex labelled k and all associated edges Ck,j<k , CiJHi
(short for any Clw' where j < k, resp. Ci,k where k < i), can be eliminated from
the graph by using the following procedure. We take each out—edge labelled CiJHi
leaving vertex k and each in—edge Ck,j<k entering and add the product Ci,k * Clw' to
the edge 01'ij creating a new edge CM if required. The edges Ct,k<tzck,j<k may now
be removed from the graph since their contributions to the Jacobian calculation
have been added to other edges of the graph.

: 07,4 * 04,2 + C7,2 + 07,5 * C5,2~

EXAMPLE 3.1 VERTEX ELIMINATION IN THE COMPUTATIONAL GRAPH.
Verter4 in Figure 1(a) has two in—edges C471, C472 and two out—edges C774, c674.
Consequently, there are four products of out—edges with in—edges 6774*6471, 0774*6472,
c674 * C471 and c674 * C471. Of these products, two are added to eTisting edges of the
graph C772 : C772 + C774 * C472, c672 : c672 + c674 * C472, and two require new edges
(shown in bold in Figure 1(b)) to be added, c671 : c674 * C471, C771 : C774 * C471.
Once these operations have been performed, verter4 and its associated edges may
be removed leaving the graph of Figure 1(b).

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

October 11, 2004 Jacobian Code by Vertex Elimination ' 277

Comparison of the operations of Example 3.1 with the ﬁrst Gaussian elimination
performed on the extended Jacobian of Section 3.1 demonstrates the equivalence
of the two interpretations, matrix and graph, of vertex elimination. Figures 1(0)
and 1(d) show the graph after elimination of vertices 5 and 6 respectively. The nu—
merical operations involved to update edge labels are precisely those of Section 3.1
used to update the extended Jacobian entries. Figure 1(d) shows the graph after
eliminating all intermediate vertices and associated edges. We see that the graph
is bipartite, i.e. the vertices form two disjoint sets, independent and dependent,
and the only edges go from an independent to a dependent. Each edge’s label
corresponds to a desired partial derivative.

Any entries cm- in the ﬁnal m columns of C correspond to edges between depen—
dent variables. Their elimination (see ﬁnal paragraph of Section 3.1) corresponds
to elimination of edges. For our example, if there is an edge c877 and it is eliminated
last, all the edges to node 8 must be modiﬁed: c871- : c871- + C7,,- * c877, i : 1,2,3.

3.3 The Connection to Standard AD Algorithms

There is a close relationship between the elimination algorithm with the pivots taken
in forward order, and the Forward Mode of AD, equivalent to solving the system
(5) by forward substitution (Section 2.2). We may write the extended Jacobian
C * I in the equivalent block form [Griewank 2000, p. 22],

[71,, 0 0]
04:73 LEI], 1(1) I7 (17)

with L strictly lower—triangular. We must now solve,

iii“ L91. 3 l l§iil _l1§“l. (18)

IR T aImllwl’lol

Forward substitution, as for conventional forward mode AD, gives
Vx : In,
vw : 7(L a Iprlex : 7(L 7 1,9713,
Vy va + va : R a T(L 7 1,9713.

In the elimination approach, we eliminate subdiagonal entries in the L block and
all entries in the T block from (18),

i , :12. 5’ ° ilV:i_i’I“i
Haifa S: a 12,171 3 I’ “9)

to leave the Jacobian in the lower—left block. The two approaches are thus alge—
braically equivalent [Griewank 2000, Section 8.1]. We also see that in the forward—
substitution approach ﬁll—in is conﬁned to the n columns of the system right—hand
side matrix, whereas in the elimination approach it is conﬁned to the ﬁrst n columns
of the extended Jacobian. Note that the matrix (L * Ip) is lower triangular and so
the product (L*Ip)71B is determined by forward substitution and not by inverting
the matrix and multiplying.

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

278 ' Shaun A. Forth et al. March 8th 2004

Similarly, the Reverse Mode is equivalent to elimination with the pivots taken in
reverse order. This conﬁnes ﬁll—in to the R and T blocks. It is equivalent to solving
the transposed system (10) by back—substitution. For our example, this technique
also requires 12 ﬂops to calculate the J acobian.

If full advantage of sparsity is taken, then [Griewank and Reese 1991] showed
that the forward and reverse ordered eliminations will never use more ﬂoating—point
operations than conventional forward and reverse mode AD, even when conventional
techniques use features such as J acobian compression or a sparse representation of
gradients and adjoints.

3.4 Elimination Sequences

From the above discussion it is clear that, instead of using the elimination approach
for calculating Jacobians, we could simply implement forward and reverse mode
AD and explicitly account for the sparsity of directional derivatives or adjoints.
However, an advantage of the elimination approach is that it removes the restriction
to monotonic increasing or decreasing pivot orderings and some other pivot sequence
might be chosen. The use of non—monotonic pivot sequences is referred to as cross—
country elimination [Griewank 2000, Chap 8.] and the extra degrees of freedom so
gained give scope for reducing the computational cost. However, we do note that
the forward and reverse orderings have the advantage of conﬁning ﬁll—in to blocks
B and R or R and T. Another pivot ordering is likely to create ﬁll—ins in block L.
This block is usually far larger than B, R or T, since there are usually far more
intermediate variables than independent or dependent variables. There is therefore
a danger of producing a large number of ﬁll—ins which must be removed later in the
elimination process and which would compromise efﬁciency.

The problem of choosing a good pivot sequence for Gaussian elimination of sparse
matrices has been well studied [Duff et al. 1989]. One of the earliest pivot ordering
heuristics is due to [Markowitz 1957]. The Markowitz heuristic involves, at each
elimination step, choosing the next pivot to minimize the product of the number
of other entries in its row and the number of other entries in its column. This
product, known as the Markowitz cost, is an upper bound both on the ﬁll—in at the
next elimination step and on the number of multiplications required (noting that
pivots are all *1 and that some of the other entries may have the value 1 or *1).
Although the Markowitz heuristic is often very successful in reducing the operations
count, there are simple examples [Griewank 2000, p. 179] for which it is found to be
sub—optimal. In his thesis, [Naumann 1999] suggested the use of what he called the
VerteT Lowest Relative (VLR) heuristic also termed Relatively Greedy Markowitz
by [Griewank 2000, p. 179]. This VLR heuristic cost is the difference between the
current (Markowitz) cost of a candidate pivot and the cost of eliminating it last in
the elimination sequence. We have previously noted [Tadjouddine et al. 2001] that
the VLR heuristic tends to eliminate those intermediate variables calculated near
the start or end of the code after those in the middle.

In the event of two or more candidate pivots having an equal minimum Markowitz
or VLR cost, then a tie—breaking strategy must be used. [Griewank and Reese 1991]
selected the candidate that resulted in most entries in the extended Jacobian being
removed. In our work, we choose the last pivot with minimum cost [Tadjouddine
et al. 2001].

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

October 11, 2004 Jacobian Code by Vertex Elimination ' 279

3.5 Roundoff

The accuracy of automatic differentiation necessarily depends on the accuracy of
the calculation of the function and its local derivatives CM" If this is ill conditioned,
as for example when the “wrong” method for the calculation of a small root of a
quadratic equation is employed, the derivative calculation will be ill conditioned
too. However, it is important to know if damage might be inﬂicted by a poor
choice of elimination sequence. One of us [Reid 2003] has considered this.

The forward method is just forward substitution applied to the system (5) and
standard backward error analysis [Wilkinson 1965, pp. 247—248] shows that for each

column of P we will have solved a nearby problem
(C+6C*IN)d: *p, (20)

with [6cm] S (3r/2 + 3)]ciyj|, where r is the largest number of entries in a row of
C. Although the perturbations will differ from column to column, we will in each
case have done no worse than we would have done for an exact calculation for very
slightly perturbed local derivatives CW" The same very satisfactory result holds for
the backward method since it is back—substitution applied to the system (10).

Unfortunately, [Reid 2003] has found an (admittedly artiﬁcial) example that
shows that another pivot sequence can be unstable. If the entries of C are

02k,2k*1 = 20, 02k+1,2k*1 = 20, C2k+1,2k = 71-07 k :1727-uili (21)

the pivot sequence 1,3, . . . , 2l * 1, 2, 4, . . . , 2l, leads approximately to the doubling
of the size of matrix entries at each stage until 2l * 1. The matrix C * I is not
ill—conditioned, so the ﬁnal solution is not large, but large rounding errors will have
occurred in intermediate stages.

While this negative result leads us to be cautious, we have not encountered
instability in practical cases.

3.6 Statement—Level Differentiation

Now consider the statement—level differentiated version of the code, ignoring the
possibility of array operations permitted, for example, in the Fortran 95 language.
One approach, adopted by TAMC [Giering and Kaminski 1998], is to differentiate
each statement symbolically. This has the potential disadvantage of generating local
derivative code with many common sub—expressions: though we would expect these
to be eliminated by today’s optimising compilers. Alternatively, the right—hand side
of a statement may be regarded as composed of the corresponding several lines of
the code list, eventually assigning one value to the left—hand side. To obtain the
local derivatives cm- associated with the statement, it is a well established technique
to differentiate the local code list using reverse mode AD. This strategy is used by
ADIFOR [Bischof et al. 1998] within an overall forward mode AD approach.

3.7 Reverse Pre—Elimination

One way to order the eliminations within the extended Jacobian of the code—list is
to follow ADIFOR by starting with reverse—ordered elimination of the set of interme—
diate variables within a statement. This may be seen as a sequence of eliminations
of intermediate variables with single successors. This is desirable since each such

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

280 ' Shaun A. Forth et al. March 8th 2004

elimination reduces the number of entries in C by at least one. To see this, sup—
pose the variable has k predecessors and a single successor, consider the square
submatrix of order k + 2

Cu * Ik
C21 *1 (22)
C31 C32 *1

corresponding to all the variables involved where the variable itself is in the middle.
There can be no ﬁll outside this submatrix since there are no entries outside it in
the row and column of the intermediate variable. Furthermore, all ﬁll takes place
within C31. Because of our choice of variables, C21 is a full submatrix of order
lxk and C32 is a nonzero 1x1 submatrix. We therefore lose [6+1 entries when we
discard the pivot row and column and end with k entries in C31. Thus the number
of entries is reduced by at least one (more if C31 starts as nonzero).

This suggests that it is desirable to eliminate any intermediate variable with a
single successor. We have therefore implemented a reverse pre—elimination strategy
in which we repeatedly consider all intermediate variables starting from the last
and proceeding to the ﬁrst and successively eliminate each one encountered which
has a single successor. Note that this will include the elimination from the code—list
extended J acobian of all intermediate variables from within any statement and so
mimics the statement—level reverse strategy of ADIFOR [Bischof et al. 1996]. Reverse
pre—elimination will also eliminate any variable used in the right—hand side of only
one other statement and so reduce the number of arithmetic operations, both in
the code—list and the statement—level extended J acobian. In particular, the result of
any unary operation will be eliminated, a process referred to as hoisting in [Bischof
1991].

The elimination AD techniques described above have been implemented in our
source transformation AD tool ELIAD which we now describe.

4. THE ELIAD TOOL

ELIAD is a vertex elimination AD tool, written in Java and uses a front—end (parser)
and back—end (pretty—printer) generated by ANTLR [Parr et al. 2000]. Though we
still regard ELIAD as a proof—of—concept tool, it is available from the authors. It
uses source—transformation to convert Fortran code for a function f into Fortran
code for f and its Jacobian. ELIAD performs a bi—directional data ﬂow analysis
to determine active variables from user speciﬁed independent and dependent vari—
ables [Hascoet et al. 2003]. It then performs a symbolic differentiation of each
statement of the function (or its code—list) to obtain each subdiagonal entry CM
of the extended Jacobian. All such entries, and those generated by ﬁll—in during
elimination, become separate (scalar) Fortran variables, e.g. C771 might appear in
the code as the variable C_7_1. Each elimination operation, following a chosen pivot
sequence, becomes a set of separate Fortran statements, e.g. the ﬁll—in C771 : 07746471
becomes

c_7_1 = C_7_4*c_4_1

Work is in hand to improve ELIAD’s ability to suppress the multiplication if one
of the operands is known a priori to be i1 and perform other such optimizations.

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

October 11, 2004 Jacobian Code by Vertex Elimination ' 281

ELIAD allows arrays as input arguments provided their indexing is static, that
is, can be calculated a priori. In effect, ELIAD unrolls them. For instance, if the
inputs are two arrays each of length 5, then their elements become input variables
:31 to $5 and :36 to 5310, respectively.

ELIAD allows branching [Tadjouddine et al. 2003], though no such test problems
are considered in this paper. Results for one test problem may be found in [Forth
and Tadjouddine 2003]. Currently no kind of loop is supported.

The chief advantage of this approach is that it permits sparsity in the elimination
to be exploited to the maximum. A disadvantage is that the sparsity pattern of
the matrix C (or a modest overestimate) must be known a priori. Also, though
the generated code runs very fast, its length is roughly proportional to the number
of elimination operations, which may be expected to grow more than linearly in
the length of the f code. However, as we shall see in Section 6, ELIAD has been
applied successfully to loop—free subroutines with between 8 to 134 independent, 5
to 252 dependent and over 1000 intermediate variables.

After ELIAD has built the extended Jacobian, it applies some algorithm *
currently an external program * to determine a good pivot sequence using the
Markowitz or a related heuristic, and uses this to generate the Jacobian code.

4.1 Generating the Jacobian Code

Using the abstract syntax tree of the input code [Aho et al. 1995] and associated
symbolic information, ELIAD intersperses the original code with assignments that
compute local partial derivatives statement by statement. Then, using the given
elimination sequence, it generates a series of scalar assignments that eliminate all
coeﬂicients of intermediate variables from the extended J acobian. Each statement
computes a new entry or updates an existing entry of the extended Jacobian as
may be seen in the following example.

EXAMPLE 4.1 JACOBIAN CODE. The Jacobian code using the reverse ordering
* that is, eliminating intermediate variables u6, u5 and u4 in that order * from
the ﬁve non—trivial assignments of Emample 2.2 could be built up as follows:

! Calculate the values of the variables and local derivatives
C_4_1 = 1/u_1; C_4_2 = 1/u_2

u_4 = log(u_1*u_2)

C_5_2 = u_3**2; c_5_3 = 2*u_2*u_3

_5 = u_2*u_3**2-a

_6_2=1/u_3; C_6_3=-u_2/u_3**2; C_6_4=b

_6 = b*u_4+u_2/u_3

_7_2=-1; C_7_4=2*u_4; c_7_5=1

_7 = u_4**2+u_5-u_2

_8_5=-1; C_8_6=1/(2*sqrt (u_6))

_8 = sqrt (u_6)-u_5

! Eliminate entries in row 6

c_8_2=c_8_6*c_6_2; C_8_3=c_8_6*c_6_3; c_8_4 = c_8_6*c_6_4
! Eliminate entries in row 5

C_7_2 = c_7_2+c_7_5*c_5_2; C_7_3=c_7_5*c_5_3
C_8_2=C_8_2+C_8_5*C_5_2; C_8_3=C_8_3+C_8_5*c_5_3

! Eliminate entries in row 4

505050;:

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

282 ' Shaun A. Forth et al. March 8th 2004

C_8_1 = C_8_4*c_4_1; C_8_2=C_8_2+c_8_4*c_4_2
C_7_1 = C_7_4*c_4_1; C_7_2=C_7_2+c_7_4*c_4_2

This leaves the Jacobian’s entries in variables c_7_1, c_7_2, c_7_3, C_8_1, c_8_2,
and c_8_3.

As a ﬁnal step, ELIAD generates assignments that copy the Jacobian values into
an array before exiting the J acobian code.

There is much freedom in the order in which the above statements can be placed.
For example, C_4_1 and C_4_2 are not used until the last two lines. It was found
that strategies to reorder the Jacobian code can signiﬁcantly affect performance on
some platforms, see Section 6.1.4.

5. TEST ENVIRONMENT

We wish to compare the performance of Jacobian code produced by our vertex—
elimination AD tool ELIAD with that produced by hand and by the conventional
AD tools ADIFOR and TAMC.

We selected the ﬁve platforms described in Appendix A. We regard these as
typical of those presently in use for scientiﬁc computing. All the processors are
termed superscalar, being able to perform several instructions (e.g. adds, multiplies,
and loads from memory to arithmetic registers) in parallel via so—called pipelines.
For arithmetic to be performed in a ﬂoating—point pipeline, the necessary data must
reside in one of the small number of arithmetic registers. All the processors have
a memory hierarchy with relatively fast transfer of data between the arithmetic
registers and the level—1 cache. Required data not in the level—1 cache must be
transferred from the larger level—2 cache at a slower rate. If the data is not currently
in the level—2 cache, it must be transferred from the main memory at an even slower
rate.

The optimising compilers available for these platforms seek to maximise per—
formance by rescheduling arithmetic operations to minimise the number of data
transfers between registers and cache. A major constraint in such optimizations is
that if another instruction calls for a value not yet loaded to a register, a so—called
stall occurs and the processor must wait. The ALPHA, SGI and AMD platforms of
our study feature out—of—order erecution, in which the processor maintains a queue
of arithmetic operations so that if the one at the head of the queue stalls, it can
switch to an operation in the queue that is able to execute. As a result, the eﬂi—
ciency of code running on these highly sophisticated processors is less dependent on
compiler optimisation than for other processors. More details on such issues may
be found in [Goedecker and Hoisie 2001].

Of our ﬁve test cases, four are taken from the MINPACK—2 collection [Averick
et al. 1992] of optimization test problems. Hand—coded Jacobian code is provided.
The supplied subroutines include branching to allow for the calculation of the func—
tion alone, the function and its Jacobian, or a standard optimization start point x.
So that measured CPU times do not include the cost of the (expensive) branching,
three new subroutines were created from each original MINPACK subroutine to
perform these tasks separately. ELIAD cannot deal with loops at present, so a
PERL script was used to unroll them all. Two of the test cases (see Section 6.2.3,
Section 6.2.4) have sparse Jacobians. In these two cases all assignments of zero val—

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

October 11, 2004 Jacobian Code by Vertex Elimination ' 283

ues to the Jacobian were removed. The nominal cost in ﬂoating—point operations
W(f) was obtained from the modiﬁed MINPACK code by using a PERL script to
count the number of times that *, + , - and / operations appear in the source code.

On all platforms, it is possible to arrange compilation so that subroutines are
inlined [Goedecker and Hoisie 2001, p. 77], i.e. the compiler inserts the body of the
subroutine directly into the calling routine. Inlining removes the overhead associ—
ated with the subroutine call and so improves eﬂiciency. Inlining may dramatically
increase the size of the overall program and so may not be possible for larger sub—
routines. In our test cases, the function evaluation subroutines are suﬂiciently small
as to be readily inlined. In contrast, the Jacobian evaluation routines are larger
which typically prevents their inlining. It is usual in AD eﬂiciency analysis to ex—
amine the ratio of Jacobian to function evaluation times. Because this ratio would
be severely distorted by inlining of the function evaluation and not the Jacobian
we explicitly prevented inlining by compiling all subroutines individually and, on
our SGI platform, turning off interprocedural optimizations.

For each test problem, a driver program was written to execute and time differ—
ent Jacobian evaluation techniques. To check that the Jacobians were calculated
correctly, we compared each to one produced by the hand—coded routine if available
or ADIFOR otherwise. The checks were made outside the code that we timed to
avoid distorting the times. Each set of values for the independent variables was
generated by using the Fortran intrinsic routine randomJIumber.

The CPU timers on these processors are not able to time short executions with
good relative accuracy. It is therefore necessary to calculate many Jacobians in
each case. Simply repeating the calculation for a single x might give unreasonably
short times since it would allow more use of level—1 cache than would be possible
in a genuine application. Therefore, for each test problem and each platform, we
generated and stored many (News) vectors x and calculated J acobians for them all.

For all the test problems considered, we found that as News was increased there
came a point where the average time for a Jacobian calculation would markedly
increase. By considering the storage requirements for the sets of independent vari—
ables, dependent variables and Jacobians, we found that this increase coincided
with the storage requirements increasing beyond that of the level—2 cache. Conse—
quently, to ensure that timings are realistic, we chose News for each platform and
each problem such that all data associated with calculation and storage comfortably
ﬁts in the level—2 cache. Since this did not involve enough computation for accurate
timing, we repeated this process a number (N,epes) of times. Values of News and
N,epet used for each platform and each of the test problems of Section 6 are given
in Table XII of Appendix B.

Even with this two—level set of repeated calculations, we found that occasionally
the times varied from run to run. Usually, there were two distinct sets of times, with
little variation within each set. We believe that this effect is caused by the different
placement of arrays in memory at load time affecting the way data is moved in and
out of the caches. We therefore ran each test ten times and report the average.

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

284 ' Shaun A. Forth et al. March 8th 2004

6. TEST—PROBLEMS AND ALGORITHM ENHANCEMENTS

In Section 6.1, we describe in detail the performance of Jacobian code for the the
Roe ﬂuX CFD problem [Roe 1981], generated both by conventional means and via
ELIAD. We have considered this problem previously [Tadjouddine et al. 2002],
though not in conjunction with the pre—elimination technique of Section 3.7 which,
together with a more careful choice of News (cf. Section 5 and Table XII), has
improved the timings obtained. Section 6.2 presents the four test cases for which
hand—coded Jacobian code is available. In Section 6.3, we discuss common issues
arising from all ﬁve test cases. Note that function nominal ﬂops W(f), numbers of
lines of non—comment code (l.o.c) and measured CPU times for all test cases are

presented in Table XIII of Appendix B.

6.1 Roe Flux

Table I presents performance data for several techniques applied to calculating

Table I: Ratios of Jacobian to function ﬂop counts, number of lines of code (l,o,c,) and ratios
of Jacobian to function CPU times for Roe Flux

 

 

 

 

 

 

 

 

 

ﬂops ratio Ratios of CPU times
Technique W(Vf)/W(f) l,o,c: ALPHA SGI Ultra10 PIII AMD
ADIFOR 15.95 637 9.50 12.87 36.19 8.94 9.07
TAMC—F 21.18 499 9,79 13.15 13.11 9.65 9.80
TAMC—R 12:69 816 7:94 11:16 15:76 11:00 8.40
FD 1214 11:80 12:14 11:52 11:57 10:91
VE—SL—F 8,89 1534 4.88 6.74 14.12 8.77 5.85
VE—SL—R 7:32 1274 4:25 5:80 9:01 4.87 4.87
VE—CL—F 12.85 3175 4.59 6.50 20.56 12.16 7.49
VE—CL—R 9.50 2433 4.10 5.98 8.66 8.44 5.66
VE—SLP—F 7:85 1412 4:52 5:42 8:24 6:29 5:01
VE—SLP—R 6,78 1261 419 Q 7.58 4.55 4.50
VE—CLP—F 8,35 2123 4.71 5,49 7.74 6.95 5.27
VE—CLP—R 7:28 1917 ﬂ 5:11 7:21 4:85 4:70
VE—SLP—F—DFT 7,85 1412 4.53 5.70 9.29 5.05 5.32
VE—SLP—R—DFT 6:78 1261 ﬂ 4:97 7:11 4:55 4:71
VE—CLP—F—DFT 8:35 2123 4:43 5:74 9:21 6:81 5:74
VE—CLP—R—DFT 7,28 1971 4.07 5,45 7,24 4,69 4.96
VE—SLP—Mark 7,35 1317 4.52 5,44 8.57 5.32 4.88
VE—SLP—VLR 6:60 1222 3.96 5.27 w 433 4.38
VE—CLP—Mark 7,86 2026 4.56 5.27 9.18 6.31 5.07
VE—CLP—VLR 7:11 1933 4:12 4.75 7:65 4:44 4:56
VE—SLP—Mark—DFT 7,35 1317 4.52 5,66 9.96 4.80 4.82
VE—SLP—VLR—DFT 6:60 1222 4:33 5:07 7:68 4.20 w
VE—CLP—Mark—DFT 7,86 2026 4.57 5,13 9.41 5.63 5.32
VE—CLP—VLR—DFT 7:11 1933 4:15 5:19 6.93 4:67 4:68

 

the 5 x 10 dense Jacobian Vf(x) of the Roe ﬂuX function [Roe 1981]. For each
technique, we give W(Vf)/W(f), the ratio of the nominal number of ﬂoating—
point operations within the generated Jacobian code to those in the function code
(computed by a simple PERL script), the numbers of (non—comment) lines in the
Jacobian code (l.o.c.) and the corresponding ratios of CPU times. The operations
counts W(V(f)) and W(f) will, in general, be over—estimates of the number of

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

October 11, 2004 Jacobian Code by Vertex Elimination ' 285

ﬂoating—point operations performed since they do not take account of optimizations
performed by the compiler. In particular, the ELIAD generated Jacobian code may
contain statements assigning an entry of the extended Jacobian to be a trivial 1 or
*1 and elsewhere use this entry to multiply others. Also, such an entry may be
added to another trivial entry, resulting in a zero or non—trivial integer value. We
assume the compiler performs constant value propagation and evaluation [Goedecker
and Hoisie 2001, p. 32] to avoid such unnecessary arithmetic operations. For each
platform, the entry corresponding to the AD technique with the smallest ratio of
CPU times is highlighted in bold and any entry with a ratio that is nearly as small
is underlined.

6.1.1 Established Techniques. The ﬁrst four rows of Table I correspond to es—
tablished techniques: the use of the AD tools ADIFOR and TAMC in forward mode,
TAMC in reverse mode, and one—sided ﬁnite differencing. Speciﬁcally, ADIFOR
refers to ADIFOR 2.0D [Bischof et al. 1998] generated forward mode AD Jacobian
code using the following options: AD_EXCEPTION_FLAVOR = performance to avoid
any calls to ADIFOR’s exception handling library: and AD_SUPPRESS_LDG = true:
AD_SUPPRESS_NUM_COLS = true to ensure that all loops within the ADIFOR gener—
ated code are of ﬁxed length and hence are candidates for unrolling at high levels
of compiler optimization. TAMC—F and TAMC—R refers to TAMC generated forward
and reverse mode AD code respectively, both obtained with the option -jacobian.

6.1.2 Vertem Elimination with Forward and Reverse Orderings. The next four
rows, labelled VE—SL—F to VE—CL—R correspond to the vertex elimination (VE) AD
code generated by ELIAD. The ﬁrst pair use statement—level (SL) differentiation
and the second pair use code—list (CL) differentiation. In each of these two pairs,
the ﬁrst uses the forward (F) elimination ordering and the second the reverse (R)
ordering. It is seen that for each platform, with the exception of the Ultra10, the
best of these four results is about twice as fast as the best established technique.
For this problem m < n and, as we might expect, the reverse ordered elimination
always out—performs the forward ordered with both statement—level and code—list
differentiation: TAMC is not as consistent in this respect. On the Ultra10, P111
and AMD platforms the code—list differentiated variants are often signiﬁcantly less
eﬂicient than their statement—level differentiated counterparts.

6.1.3 Reverse Pre—Elimination. In Table 1, rows VE—SLP—F and VE—SLP—R cor—
respond to a statement—level symbolic differentiation, followed by a reverse pre—
elimination (Section 3.7) and then a forward— and reverse—ordered eliminations of
all remaining vertices. Rows VE—CLP—F and VE—CLP—R are the code—list differen—
tiated equivalents. We see that reverse pre—elimination reduces the nominal ﬂops
count and subsequently improves performance for all bar one case.

6.1.4 Depth—First Traversal (DFT) Statement Re—Ordering. In Section 5, we
brieﬂy described how optimising compilers may re—schedule ﬂoating—point opera—
tions in an algebraically consistent manner, in an attempt to keep the processors
ﬂoating—point pipelines full and improve performance. We also remarked that for
those platforms that support out—of—order execution, this optimization is less im—
portant. In [Tadjouddine et al. 2002], we reported that changing the order of the

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

286 ' Shaun A. Forth et al. March 8th 2004

statements in the Jacobian code generated by ELIAD could dramatically affect the
performance of the code on platforms that do not support ﬂoating—point out—of—
order execution. We conjectured, and showed for one example, that this was due
to better instruction scheduling resulting in fewer reads and writes from cache to
registers and hence fewer stalls in the ﬂoating—point pipelines.

To assess the impact of statement ordering in the derivative code, we reordered
the assignment statements without altering the data dependencies within the code,
with the aim of using each assigned value soon after its assignment. This was done
by a using modiﬁed version of the depth—ﬁrst traversal (DFT) algorithm [Knuth
1997]. Namely, we regarded the statements in the derivative code as the vertices
of an acyclic graph, with the output statements at the top and an edge from s to
t if the variable assigned by statement 5 appears in the right—hand side expression
in statement t. Then, we arranged the statements in the postorder produced by a
depth—ﬁrst traversal of this graph.

Rows VE—SLP—F—DFT to VE—CLP—R—DFT of Table I show the result of employing
our DFT strategy to the codes of rows VE—SLP—F to VE—CLP—R. In contrast to
previous results [Tadjouddine et al. 2002], obtained without pre—elimination, we see
that the results of employing DFT are mixed. For example, timings on the P111
platform are improved but those on the SGI and AMD are worsened.

6.1.5 Cross—Country VerteT Elimination. Rows VE—SLP—Mark to VE—CLP—VLR
of Table I show the result of employing the Markowitz and VLR heuristics of Sec—
tion 3.4 to order the vertex elimination, after the reverse pre—elimination of Sec—
tion 3.7. The resulting VE—SLP—VLR has the lowest nominal ﬂop count obtained
for this problem and is the fastest executing on the ALPHA and AMD platforms.
The VE—CLP—VLR case also has a low nominal ﬂop count and is fastest on the SGI.

Employing DFT reordering to the cross—country elimination J acobian code (rows
VE—SLP—Mark—DFT to VE—CLP—VLR—DFT) again had mixed results. However,
on the Ultra10 and P111 platforms use of DFT ensures that one of the low ﬂop
cross—country techniques, VE—CLP—VLR—DFT on Ultra10 and VE—SLP—VLR—DFT
on P111, becomes the fastest.

6.2 Further Test Cases

Given the encouraging results for the Roe case, we now consider four further test
cases taken from [Averick et al. 1992] and for which hand—coded Jacobians are
available.

6.2.1 HHD — Human Heart Dipole. Table 11 presents performance data for cal—
culating the dense 8 x 8 Jacobian of this test problem. The same techniques are used
as in Section 6.1 with the addition of a row for the hand—coded Jacobian results.
Note that Markowitz and VLR orderings when applied to either the statement—level
or code—list differentiation produced equivalent elimination sequences (the codes dif—
fered only in statement ordering) and so only the Markowitz results are shown.

6.2.2 CPF — Combustion of Propane (Full Formulation). Table 111 gives data
for calculating the 53 entry, 11 x 11 Jacobian of the CPF problem. The Markowitz
and VLR orderings are equivalent to the forward ordering and so not shown.

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

October 11, 2004

Table II:

Jacobian Code by Vertex Elimination '

287

Ratios of Jacobian to function ﬂop counts, numbers of lines of code (l,o,c,) and ratios

of Jacobian to function CPU times for Human Heart Dipole problem:

 

 

 

 

 

 

 

 

 

 

ﬂops ratio Ratios of CPU times
Technique W(Vf)/W(f) l,o,c: ALPHA SGI Ultra10 PIII AMD
Hand—coded 2:00 205 2:97 3:88 4:79 2:22 2:64
ADIFOR 13:88 229 15:30 21:26 18:61 14.59 13.80
TAMC—F 18:31 196 16:93 24:27 24:85 16:62 14:90
TAMC—R 20.79 324 20.25 37.82 27.07 26.36 23.09
FD 9:19 12:26 12:58 13:46 10:90 12:28
VE—SL—F 3,05 362 3.05 4.28 3.75 3.32 3.82
VE—SL—R 3.00 356 3.26 3.92 3.83 3.35 3.79
VE—CL—F 5:00 826 2:92 4:50 4:01 3:83 4:26
VE—CL—R 3:95 721 2:79 4:01 3:52 3:74 4:33
VE—SLP—F 3,05 348 2.75 3.83 3.50 3.21 3.73
VE—SLP—R 3,05 349 2.61 3.76 3.43 3.29 3.70
VE—CLP—F 3.90 706 2.79 3.90 3.80 3.61 4.24
VE—CLP—R 3.90 707 2.79 M 3.19 3.63 4.09
VE—SLP—F—DFT 3,05 348 2.99 3.80 3.54 g 3.52
VE—SLP—R—DFT 3,05 349 E 3.71 3.88 3.28 3.53
VE—CLP—F—DFT 3,90 706 3.19 3.88 3.86 3.56 3.80
VE—CLP—R—DFT 3:90 707 3:25 3:93 3.19 3:43 3.46
VE—SLP—Mark 3,00 344 2.70 3.85 3.36 3.26 3.69
VE—CLP—Mark 3:86 701 2:82 3:91 4:02 3:83 4:21
VE—SLP—Mark—DFT 3,00 344 2,99 3,81 3.53 3.29 3.54
VE—CLP—Mark—DFT 3,86 701 3.17 3.88 3.86 3.43 3.77

 

Table III,

Ratio of Jacobian to function ﬂop counts, number of lines of code (l,o,c,) and

ratio of Jacobian to function CPU times for Combustion of Propane (full formulation)

 

 

 

 

 

 

ﬂops ratio Ratios of CPU times
Technique W(Vf)/W(f) l,o,c: ALPHA SGI Ultra10 PIII AMD
Hand—coded 2:24 237 1:97 2:42 3:66 2:23 2.86
ADIFOR 14.44 517 5,89 6.63 10.69 5.76 8.81
TAMC—F 24:76 115 6:60 7:41 11:52 6:95 12:92
TAMC—R 27.03 163 9.49 10.49 19.85 9.85 12.53
FD 1556 — 14:56 13:70 14:15 13:43 14:42
VE—SL—F 3:04 342 1:77 1:91 2:53 2:15 3:12
VE—SL—R 2:41 291 1:82 1:94 2:60 2:23 3:14
VE—CL—F 3:81 607 1:77 1:91 2:55 2:23 3:24
VE—CL—R 2:78 523 1:68 1:89 2:35 2:18 3:11
VE—SLP—F 2:37 225 1:74 1.49 1:93 2:02 2:94
VE—SLP—R 2:40 226 1:66 1:64 1:84 2:04 2:93
VE—CLP—F 2,75 453 1.62 w 187 1.97 2.86
VE—CLP—R 2,78 455 1,64 1,60 1,85 1.98 2.86
VE—SLP—F—DFT 2:37 225 1:37 1:53 1:85 & 2.27
VE—SLP—R—DFT 2:40 226 ﬂ 1:52 1:95 1:88 2.27
VE—CLP—F—DFT 2:75 453 1.32 1:72 1:89 1.84 2:32
VE—CLP—R—DFT 2:78 455 1:35 1:54 1.81 1.84 E

 

 

 

6.2.3 CTS — Coating Thickness Standardization. The CTS problem has a 252 x
134 sparse Jacobian with a maximum of 6 entries per row and 63 per column. The
coding is such that a statement—level differentiation has no intermediate variables,
that is, p : 0. In contrast, the code—list uses p : 1386 intermediates. Table IV

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

288 ' Shaun A. Forth et al. March 8th 2004
gives the nominal ﬂop and CPU ratios as for our previous examples, except that

Table IV: Ratios of Jacobian to function ﬂop counts: numbers of lines of code (l,o,c:) and
ratios of Jacobian to function CPU times for Coating Thickness Standardization

 

 

 

 

 

Ratios of ﬂops Ratios of CPU times
Technique W(Vf)/W(f) l,o,c: ALPHA SGI Ultra10 PIII AMD
Hand—coded 1,85 1419 4.62 3.79 9.63 3.89 3.18
ADIFOR(cmp) 6,38 2311 37,42 48.84 75.80 25.40 30.23
TAMC—F(cmp) 11.15 1688 33.19 48.90 74.81 25.40 30.96
FD(cmp) 6,00 33,35 32.52 49.91 19.73 27.01
VE—SL 1:85 2468 4.07 4:12 7.28 4:30 3:09
VE—CL—F 3.23 11160 w M 9.48 5.47 4.08
VE—CL—R 2,54 9800 4.16 4.06 9.66 5.20 3.78
VE—SL—DFT 1:85 2468 4:85 4:86 9:96 3.78 2.60
VE—CL—F—DFT 3.23 11160 4:75 4:83 8:48 4:36 3.11
VE—CL—R—DFT 2,54 9800 4.74 4,82 9,31 4,36 2.87

 

 

 

compression is used for the conventional methods: ADIFOR(cmp), TAMC—F(cmp)
and FD(cmp). We use the DSM software [Coleman et al. 1984] to obtain a column
compression for the supplied sparsity pattern allowing the Jacobian to be recon—
structed from q : 6 Jacobian—vector products. Note that since we unroll loops
prior to differentiation for ELIAD’s beneﬁt, then for consistency we apply ADIFOR
and TAMC to the function coding after loop—unrolling. In practice, this made little
difference to CPU times but does lead to a large number of lines of code as seen
in Table IV. The application of ADIFOR with the SPARSLINC library for sparse
storage of gradients was tried on the ALPHA and Ultra10 platforms, but results
were disappointing being approximately 10 times slower than compressed ﬁnite
differencing.

The row labelled VE—SL refers to vertex elimination AD with differentiation
at the statement level. Since there are no intermediate variables for statement—
level differentiation of this problem, as explained above, the partial derivatives
of each statement correspond to entries in the Jacobian. With reference to (17),
the sub—blocks B, L and T of the extended Jacobian are empty and sub—block R
is the required function Jacobian. No linear solve is required, W(linear solve) :
0, and the computed statement partial derivatives are inserted directly into the
Jacobian. The equality of “Ratios of ﬂops” for Hand—coded and VE—SL indicates
their equivalence. This is not true for conventional AD. For example, in forward
mode AD the coeﬂicients R multiply the length n (or length q for compression)
vectors of the in, i: 1, . . . ,n.

The reverse pre—elimination of Section 3.7 is not necessary on this problem since
it will produce the same elimination sequence as the reverse ordering [Naumann
1999, p. 55]. No cross—country elimination sequences were used for this problem
since use of reverse pre—elimination alone would allow for J acobian evaluation.

6.2.4 FIC — Flow in Channel. For this problem the supplied subroutine uses
p : 680 intermediate variables and its code—list p : 1328. The resulting 32 x 32
Jacobian is sparse with a maximum of 9 nonzeros per row and per column. Table V
gives the nominal ﬂop and CPU ratios as before. For the established techniques,

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

October 11, 2004 Jacobian Code by Vertex Elimination ' 289

Table V. Ratios of Jacobian to function ﬂop counts, numbers of lines of code (l.o.c.) and
ratios of Jacobian to function CPU times for Flow in Channel

 

 

 

 

 

Ratios of ﬂops Ratios of CPU times
Technique W(Vf)/W(f) l.o.c. ALPHA SGI Ultra10 PIII AMD
Hand—coded 1.91 1786 2.44 2.72 8.33 3.34 2.58
ADIFOR(cmp) 10.44 2989 15.50 57.93 38.48 47.68 59.52
TAMC—F(cmp) 10.85 3508 15.54 56.66 38.57 46.84 57.95
FD(cmp) 9.20 15.06 17.61 18.59 30.24 17.42
VE—SL—F 3.49 6300 2.21 3.08 3.82 4.18 2.78
VE—SL—R 2.25 4420 M 3.10 4.05 5.12 3.66
VE—CL—F 4.44 9982 2.33 3.26 4.94 4.87 3.35
VE—CL—R 2.75 7411 Q 3.09 3.41 5.17 3.33
VE—SL—R—DFT 2.25 4420 2.10 2.97 4.89 4.72 2.67
VE—CL—R—DFT 2.75 7411 w Q 6.80 4.91 2.56

 

 

 

we again use the DSM software to enable row compression of the Jacobian cal—
culation using q : 9 Jacobian—vector products and the function is unrolled prior
to differentiation. An interesting feature of the FIC problem is that each of its
intermediate variables is used in only one other statement. Consequently, reverse
pre—elimination alone eliminates all intermediate variables and is equivalent to the
reverse orderings VE—SL—R and VE—CL—R. Because of this, no pre—eliminated results
are shown. We also give results for the DFT code—reordering applied to these two
cases. No cross—country elimination results are shown since they could not improve
on pre—elimination.

6.3 Discussion of the Results

We now discuss the results of Tables I to V.

6.3.1 Forward and Reverse VerteT Elimination. In Tables I to V, the ratio ofthe
nominal operations count W(V(f))/W(f) for rows VE—SL—F to VE—CL—R indicates
that forward and reverse vertex elimination should give much improved performance
compared to the established techniques and this is conﬁrmed in the run—time ratios.
Indeed on the ALPHA and Ultra10 platforms, the vertex elimination techniques
almost always out—perform the hand—coded Jacobian code, where available, and the
best always does so.

6.3.2 Forward and Reverse VerteT Elimination with Reverse Pre—Elimination.
The difference in nominal ﬂops between the code—list and statement—level differenti—
ated J acobian codes may be greatly reduced by using reverse pre—elimination (Sec—
tion 3.7). For example, compare VE—CL—F/VE—CL—R and VE—CLP—F/VE—CLP—R
in Tables I to II. The reduction is particularly noteworthy for the forward order—
ings (VE—CL—R, VE—CLP—F) of these tables. Pre—elimination may also improve a
statement—level differentiated code in cases where an active intermediate variable
of the original function code is only used once. See, for example, Table I.

In terms of run—time, the application of pre—elimination usually improves eﬂi—
ciency. Sometimes the improvement is very substantial (see Table I, Ultra10 and
PIII) and sometimes there is hardly any change despite a worthwhile reduction
in the number of operations (see Table I, ALPHA). We believe this is associated
with how well the compiler optimizes the code. We see that reverse pre—elimination

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

290 ' Shaun A. Forth et al. March 8th 2004

 

20-

Fig. 2. 40' \

Variable usage in .
the CPF Jacobian code ‘I I
VE-SLP-F. Each row cor— 60 3‘ 1 I
responds to a statement s
and its entries indicate the 80- i t 3 . .
statements that calculated ll. ' I ' .‘
the values of the variables ' '0 a . _
involved. 100' ' I u

120- .

 

 

.
140- . . . . '5

 

rarely increases run time. We conclude that this is a very worthwhile strategy to
use.

6.3.3 Depth—First Traversal (DFT) Statement Re—Ordering. The results with
depth—ﬁrst traversal (DFT) statement re—ordering (Section 6.1.4) were very mixed.
Signiﬁcant gains were sometimes made: ALPHA and AMD of Table III, P111 and
AMD of Table IV, P111 and AMD of Table V. Sometimes signiﬁcant losses were
made: ALPHA, SGI of Table IV, Ultra10 of Table V. When successful, DFT ap—
pears to act as desired by reducing the number of memory operations performed.
For example, in Table V the application of DFT re—ordering to the VE—SL—R sub—
routine to give VE—SL—R—DFT results in an 8% speed—up on the P111 platform. On
examining the associated assembler we ﬁnd that this is most likely to be due to a
14% drop in the number of loads and 8% drop in the number of stores. Conversely,
on the Ultra10 platform we get a 21% reduction in eﬂiciency on applying DFT and
this is found to be associated with an undesired 27% increase in the number of
stores.

To understand how DFT reordering can reduce reads and writes to and from
cache, consider the CPF problem with results of Table 111. With only four excep—
tions, application of DFT reordering improves performance. Figures 2 and 3 show
within which statements of the Jacobian codes VE—SLP—R and VE—SLP—R—DFT the
variables are used. The ﬁrst empty 29 rows in both plots correspond to the 11 inde—
pendent variables and 18 constants (which are not calculated). Entries in columns
1 to 29 therefore denote uses of these variables and constants. In Figure 2, rows 30
to 62 correspond to the calculation of the function and local derivatives in the VE—
SLP—R code: rows 63 to 87 correspond to the vertex elimination of intermediates
from the function’s extended Jacobian: and rows 88 to 141 represent the assignment
of the calculated scalar Jacobian entries to their matrix storage. Note that several
columns are empty and correspond to variables unused in the right—hand side of any
other statement e.g. assignments to the Jacobian. We see that the statements us—
ing a variable’s value are often widely separated from the statement that calculated

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

 

 

 

 

October 11, 2004 Jacobian Code by Vertex Elimination 291
0
20
I a. a.
40 .v.
I_« Fig. 3. Variable usage in
,.g_ e I the the DFT reordered CPF
50 e 2 . Jacobian code VE-SLP-F-DFT.
. . Each row corresponds to a
80 . statement and its entries indi—
' .. . ' cate the statements that cal—
” 0' . 'o culated the values of the vari—
n n
100 3' ..0 . o. ' ables involved.
0
u. I... ...
120 '- ' '.
140 a} ' ' . ° \ _
0 20 40 60 80 100 120 140

that value. For example, statement 141 uses the result of statement 86 — separated
by 55 other statements. Unless the compiler itself performs signiﬁcant reordering
of the arithmetic instructions of the Jacobian code, then to keep all variables in
registers between their calculation and last use will require a large number of reg—
isters. In Figure 3 we see that, with only a handful of exceptions, DFT reordering
ensures entries of rows 30 to 141 lie either in the ﬁrst 22 columns, corresponding to
uses of independent variables and constants, or are clustered close to the diagonal
corresponding to use of a value shortly after its calculation. Consequently if the
independents are ﬁrst read from cache and, together with the problem constants,
kept in 22 registers throughout, then only a small number of other registers will be
needed to avoid reads and writes to cache other than those necessary to store the
function values and J acobian entries.

In a signiﬁcant number of cases our best result (highlighted in bold) was obtained
with a DFT code, which means that the technique should not be dismissed despite
its mixed performance. The technique clearly needs improvement. Its most appar—
ent weakness is that it fails to account for multiple uses of the independent variables
and other constants. If, as in Figure 3, such uses are distributed throughout the
code, and the numbers of such variables and constants is close to or exceeds the
number of registers available, then performance will be impaired by the many loads
and stores required.

6.3.4 Cross—Country Vertem Elimination. Only Table I shows operation count
gains from cross—country vertex elimination that are other than negligible. In Ta—
ble 1, across all platforms, the best results were obtained by cross—country vertex
elimination, sometimes aided by DFT statement re—ordering.

Why it is only for the Roe case that cross—country elimination produces bene—
ﬁts demands some explanation. Table VI shows the number of entries in each of
the blocks of the extended Jacobian as deﬁned by equation (18) before and after
pre—elimination. For the two sparse cases CTS and FIC of Sections 6.2.3 and 6.2.4,
pre—elimination alone obtains the J acobian in block R with other blocks then empty.

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

292 ' Shaun A. Forth et al. March 8th 2004

Table VI. Size and number of entries in blocks B, L, R and T of the extended
Jacobians (equation (18)) for test cases differentiated at statement level. Figures
in brackets are after reverse pre—elimination

 

 

 

 

Size Number of entries in block
Problem n m p B L R T
Roe 10 5 62(36) 39(39) 140(88) 5(5) 14(37)
HHD 8 8 18(18) 8(8) 16(16) 0(0) 68(68)
CPF 11 11 12(2) 12(1) 10(1) 38(49) 6(5)
CTS 134 252 0(0) 0(0) 0(0) 882(882) 0(0)
FIC 32 32 582(0) 582(0) 486(0) 16(246) 96(0)

 

For the CPF problem of Section 6.2.2, pre—elimination eliminates all but 2 inter—
mediates, so there can only be two subsequent, distinct elimination sequences for
the statement—level differentiated code, namely forward and reverse orderings. This
explains why use of the Markowitz and VLR heuristics leads to no improvement in
the number of ﬂops. After pre—elimination, the HHD problem of Section 6.2.1 still
has 18 intermediates and so there is scope for the Markowitz and VLR heuristics
to improve eﬂiciency. A small reduction in nominal ﬂops is obtained, though the
two sequences are equivalent and performance is not signiﬁcantly improved.

However, for the Roe problem of Section 6.1, 36 intermediates and 88 entries in
block L remain after pre—elimination. This greater complexity gives scope for the
Markowitz and VLR heuristics to have a greater impact on the number of ﬂops.
Hence only for this case do the heuristics result in the most eﬂicient J acobian code,
even so the improvement is small.

6.3.5 Code—List versus Statement—Level Diﬁ‘erentiation. In Tables I—V we see
that the pre—eliminated statement—level differentiated vertex—elimination codes al—
ways use fewer nominal ﬂops and, in 38 of the 55 distinct problem/platform com—
binations (excluding DFT reorderings), out—perform the corresponding code—list
differentiated codes.

6.4 Overall Performance of Vertex—Elimination AD

Given the many variations in Jacobian code produced by ELIAD (statement—
level or code—list differentiation, pre—elimination, elimination orderings and code
re—orderings) it is useful to summarise the performance of the best ELIAD gener—
ated Jacobian code with respect to the other calculation methods.

Table VII gives the speed—up obtained in moving from l—sided ﬁnite differencing
(using row compression where applicable) to our best vertex elimination method
for each problem and each platform. The inherent truncation error associated with
ﬁnite differencing and the superior eﬂiciency of factors between 1.7 to 11 for the
ELIAD generated code appear to justify use of elimination AD over ﬁnite differ—
encing, though the eﬂiciency of ﬁnite differencing would be improved by allowing
the compiler to inline function calls.

Table VIII gives the speed—up obtained in moving from the best conventional
AD technique to our best vertex elimination method for each problem and each
platform. Speed—ups of between 1.9 to 22.8 demonstrate the superiority of vertex—
elimination AD over conventional forward and reverse mode AD for J acobian cal—
culation.

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

October 11, 2004

Jacobian Code by Vertex Elimination '

293

 

 

 

 

 

Table VII. Speed—up of best vertex elimination AD over
1—sided ﬁnite differencing
Platform
Problem ALPHA SGI Ultra10 PIII AMD
Roe 3.0 2.6 1.7 2.8 2.5
HHD 4.7 3.4 4.2 3.4 3.5
CPF 11.0 9.2 7.8 7.3 6.4
CTS 8.2 8.0 6.9 4.6 10.4
FIC 7.2 6.2 5.5 7.2 6.9
Table VIII. Speed—up of best vertex elimination over best
conventional AD
Platform
Problem ALPHA SGI Ultra10 PIII AMD
Roe 2.0 2.3 1.9 2.1 1.9
HHD 5.9 5.7 5.8 4.5 4.0
CPF 4.5 4.4 5.9 3.2 3.9
CTS 8.2 12.0 10.3 5.9 11.6
FIC 7.4 20.0 11.3 11.2 22.8

 

 

Table IX gives the speed—up obtained in moving from hand—coded J acobian code

Table IX.

Speed—up of best vertex elimination AD over
hand—coded Jacobian code

 

 

Platform
Problem ALPHA SGI Ultra10 PIII AMD
HHD 1.13 1.05 1.50 0.69 0.76
CPF 1.49 1.62 2.02 1.22 1.26
CTS 1.14 0.93 1.32 1.12 1.22
FIC 1.16 0.96 2.44 0.80 1.02

 

 

to our best vertex elimination method for the 4 problems for which we have hand—

coded Jacobians. Only f0

r5

of the 20 problem/ platform combinations is hand—

coding superior, and for two of these the discrepancy is within 7%.

7. CONCLUSIONS AND PLANS FOR FUTURE WORK

In this paper, we have presented the ﬁrst extended set of results from ELIAD, a
source—transformation implementation of the vertex—elimination AD approach to
Jacobian calculation of functions deﬁned by Fortran code. Careful timings demon—
strate an eﬂiciency equal to that obtained by hand—coding and between 2 to 20
times superior to conventional AD techniques. This superiority is due to ELIAD’s
fuller exploitation of the sparsity of the extended Jacobian system that governs the
relationship between the derivatives of all variables in the function code. Further,
this exploitation of sparsity is performed at the Jacobian code generation stage,
with statements generated only for those arithmetic operations involving nonzero
entries in the extended J acobian.

Vertex elimination requires an ordering, or pivot sequence, in which to eliminate
the derivatives of intermediate variables from the extended J acobian. Monotonic in—

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

294 ' Shaun A. Forth et al. March 8th 2004

creasing and decreasing orderings correspond to sparsity—exploiting variants of con—
ventional forward and reverse mode AD [Griewank and Reese 1991]. Consequently,
vertex—elimination AD never uses more ﬂoating—point operations than conventional
AD. The use of cross—country (i.e. non—monotonic) orderings gives further scope for
reducing the number of ﬂoating—point operations. We have found that employing
a pre—elimination strategy of eliminating any intermediate variable used only once,
followed by a forward or reverse ordered elimination of all other intermediates, is
very successful, both in reducing the number of ﬂoating—point operations and im—
proving run—times. In the future, we wish to improve this strategy in the light
of recent work [Naumann 2002b]. More involved ordering heuristics, such as the
Markowitz and VLR strategies, were only worthwhile for one test case studied to
date. Recently, elimination AD has been generalized to edge [Naumann 1999] and
face [Naumann 2001: 2002a] eliminations, which may further reduce the number of
ﬂops required for Jacobian calculation. We are currently assessing these techniques
for inclusion into ELIAD.

Interestingly, we found that reordering a Jacobian code’s statements frequently
affected its performance. This appears to be due to changes in the number of
loads and stores from cache to registers in the assembler of the reordered code. We
are currently performing an in—depth study of the assembler produced for all the
Jacobian codes of our study in order to get a better understanding of this issue and
consequently improve both elimination heuristics and our code re—ordering strategy.

For the sparse Jacobian cases (FIC and CTS), the ELIAD generated Jacobian
code was between 6 to 20 times more eﬂicient than conventional AD using Jaco—
bian compression. So, vertex—elimination AD appears excellently suited to least
squares optimisation problems and numerical PDE solvers, where eﬂicient Jaco—
bian calculation enables fast solution via Newton—like solvers. To be more generally
applicable for such problems requires removal of ELIAD’s present restriction to
loop—free code. This will greatly complicate ELIAD’s activity analysis which will
have to handle array indices as index ranges and not ﬁxed values as at present.
The techniques of [Tadjouddine et al. 1998: Tadjouddine 1999] will be modiﬁed to
enable this. The ability to store such J acobians in a suitable sparse matrix format
will also be necessary.

At the time of writing, ELIAD’s functionality is being extended to include sub—
programs. A hierarchical approach utilising a conservative activity analysis of all
variables in all possible branches and subroutines is adopted. This approach auto—
mates and generalises that of [Bischof and Haghighat 1996].

There are still many open questions related to the optimal calculation of Ja—
cobians by elimination, both theoretical and implementational. Despite this, we
feel certain that, with such issues now being studied in depth, the days of scien—
tists and engineers painstakingly hand—coding Jacobian code to ensure eﬂiciency
are numbered.

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

October 11, 2004 Jacobian Code by Vertex Elimination ' 295

APPENDICES
A. PLATFORMS

We ran test cases on COMPAQ ALPHA, Silicon Graphics and SUN UNIX machines
with processor/compiler combinations denoted ALPHA, SGI and Ultra10. We also
ran on two PC platforms with Pentium 3 and AMD Athlon processors. Relevant
hardware data and the compiler information is given in Table X.

Table X. Platforms

 

a) Processors

 

Platform I Processor CPU L1—Cache L2—Cache
ALPHA EV6 667MHz 128KB 8MB
SGI R12000 300MHz 64KB 8MB
Ultra10 SUN Ultra10 440MHz 32KB 2MB
PIII Pentium 3 700MHz 32KB 256KB
AMD Athlon XP1800+ 1533MHz 128KB 256KB

b) Compilers

 

Platform I Compiler Options

ALPHA Compaq f95 5.4 —O5 —fast —arch host —tune host

SGI f90 MIPSPro 7.3 —Ofast —IPA:inline:OFF —INLINE:none
Ultra10 Workshop f90 6.0 —fast

PIII/AMD Compaq Visual Fortran /architecture:host
/assume:noaccuracy_sensitive
/inline:manual /math_library:fast
/tune:host /opt:fast

 

B. TEST PROBLEM FUNCTION STATISTICS, CPU TIMES AND OPERATIONS
COUNTS

Table XI gives summary statistics for all 5 test problems. Columns p—SL and p—CL
refer to the number of (active) intermediate variables in each function’s original
statements and in its code—list respectively.

Table XI. Summary statistics for test
problem functions

 

 

 

Problem n m p— SL p— CL
Roe 10 5 62 208
HHD 8 8 20 84
CPF 1 1 1 1 13 5 7
CT S 1 34 252 0 1 386
FIC 32 32 680 1328

 

Table XII gives News, the number of distinct Jacobians calculated, and N,epet
the number of times these calculations were repeated, in order to obtain reliable
J acobian timings.

The second column of Table XIII gives the nominal computational cost W(f(x))
of calculating the test problems described in Section 6. It is determined by counting
the number of ﬂoating operations within the Fortran code using a simple PERL

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

 

 

 

 

 

296 ' Shaun A. Forth et al. March 8th 2004
Table XII. Values of Nevwls and Nrepet used for each problem and platform
Nevals Nrepet
ALPHA, PIII & ALPHA SGI & PIII AMD
SGI & AMD Ultra10
Problem Ultra10
Roe 500 60 2000 2000 16667 16667
HHD 2500 312 400 400 3200 12800
CPF 1000 125 2000 500 4000 16000
CTS 1 1 12000 12000 12000 120000
FIC 200 25 100 100 6400 6400
Table XIII. Nominal ﬂoating—point operations counts W(f), lines of code (l.o.c.)

and CPU times for test problem functions

 

 

W(f) Function CPU time (us)
Problem (ﬂops) l.o.c. ALPHA SGI Ultra10 PIII AMD
Roe 222 139 0.475 0.951 0.806 0.880 0.336
HHD 84 68 0.121 0.224 0.206 0.231 0.0943
CPF 68 45 0.351 0.870 0.503 0.495 0.162
CTS 1638 535 1.18 3.16 2.25 2.49 0.981
FIC 1266 759 1.53 2.93 2.99 1.94 0.878

 

 

 

script. Table XIII also gives the number of non—comment lines of code (l.o.c.) and
the average CPU times required to calculate the test problem functions f(x), as
measured using the CPU_TIME intrinsic function of the Fortran 95 programming

language.

Acknowledgements

We thank Neil Stringfellow and Venkat Sastry for their help in developing the PERL
scripts used in this paper and the three anonymous referees for the great care with

which they read our paper and for their many suggestions.

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

October 11, 2004 Jacobian Code by Vertex Elimination ' 297

REFERENCES

AHO, A. V., SETHI, R., AND ULLMAN, J. D. 1995. Compilers: Principles, Techniques, and Tools.
Addison—Wesley, Reading, Mass.

AVERICK, B. M., CARTER, R. G., MORE, J. J., AND XUE, G.—L. 1992. The MINPACK—
2 test problem collection. Preprint MCS*P153{)692, ANL/MCS*TM*150, Rev. 1, Math—
ematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill. See
ftp://info.mcs.anl.gov/pub/MINPACK-2/tprobs/P153.ps.Z.

BENDTSEN, C. AND STAUNING, O. 1996. FADBAD, a ﬂexible C++ package for automatic dif—
ferentiation. Technical Report IMM—REP—1996—17, Technical University of Denmark, IMM,
Departement of Mathematical Modeling, Lyngby.

BERz, M., BISCHOF, C., CORLISS, G., AND GRIEWANK, A., Eds. 1996. Computational Diﬁerenti—
ation: Techniques, Applications, and Tools. SIAM, Philadelphia, Penn.

BISCHOF, C., BUCKER, H., LANG, B., RASCH, A., AND VEHRESCHILD, A. 2002. Combining source
transformation and operator overloading techniques to compute derivatives for MATLAB pro—
grams. In Proceedings of the Second IEEE International Workshop on Source Code Analysis
and Manipulation (SCAM 2002). IEEE Computer Society, p. 65*72.

BISCHOF, C. H. 1991. Issues in parallel automatic differentiation. See Griewank and Corliss [1991],
100*113.

BISCHOF, C. H., CARLE, A., HOVLAND, P. D., KHADEMI, P., AND MAUER, A. 1998. ADIFOR 2.0
user’s guide (Revision D). Tech. rep., Mathematics and Computer Science Division Technical
Memorandum no. 192 and Center for Research on Parallel Computation Technical Report
CRPC—95516—S. See www.mcs.anl.gDv/adifor.

BISCHOF, C. H., CARLE, A., KHADEMI, P., AND MAUER, A. 1996. ADIFOR 2.0: Automatic
differentiation of Fortran 77 programs. IEEE Computational Science 55 Engineering 3, 3,
18*32.

BISCHOF, C. H. AND HAGHIGHAT, M. R. 1996. Hierarchical approaches to automatic differentia—
tion. See Berz et al. [1996], 83*94.

BISCHOF, C. H., KHADEMI, P. M., BOUARICHA, A., AND CARLE, A. 1996. Efﬁcient computations
of gradients and Jacobians by dynamic exploitation of sparsity in automatic differentiation.
Optimization Methods and Software 7, 1*39.

BISCHOF, C. H., ROH, L., AND MAUER, A. 1997. ADIC * An extensible automatic differentiation
tool for ANSI—C. Software * Practice and Experience 27, 12, 1427*1456. See www-fp.mcs.anl.
gov/division/software.

COLEMAN, T. F., GARBOW, B. S., AND MORE, J. J. 1984. Software for estimating sparse Jacobian
matrices. ACM Trans. Math. Software 10, 3, 329*345.

COLEMAN, T. F. AND VERMA, A. 1998. ADMAT: An automatic differentiation toolbox for MAT—
LAB. Tech. rep., Computer Science Department, Cornell University.

CORLISS, G., FAURE, C., GRIEWANK, A., HASCOET, L., AND NAUMANN, U., Eds. 2001. Auto-
matic Diﬁerentiation: From Simulation to Optimization. Computer and Information Science.
Springer, New York.

DUFF, I., ERISMAN, A. M., AND REID, J. 1989. Direct methods for sparse matrices. Monographs
on numerical analysis. Oxford University Press.

FAURE, C. AND PAPEGAY, Y. 1998. Odyssée User’s Guide. Version 1.7. Rapport technique RT*
0224, INRIA, Sophia—Antipolis, France. Sept. See www. inria.fr/RRRT/RT-0224.html, and www.
inria.fr/safir/SAM/Ddyssee/odyssee.html.

FORTH, S. A. 2001. User guide for MAD — a Matlab automatic differentiation toolbox. Applied
Mathematics and Operational Research Report AMOR 2001/5, Cranﬁeld University (RMCS
Shrivenham), Swindon, SN6 8LA, UK. June.

FORTH, S. A. AND TADJOUDDINE, M. 2003. CFD Newton solvers with EliAD, an elimination
automatic differentiation tool. In Computational Fluid Dynamics 2002: Proceedings of the
Second International Conference on Computational Fluid Dynamics, ICCFD, S. Armﬁeld,
P. Morgan, and K. Srinivas, Eds. Springer, Sydney, Australia, 134*139.

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

298 ' Shaun A. Forth et al. March 8th 2004

GIERING, R. AND KAMINSKI, T. 1998. Recipies for adjoint code construction. ACM Trans. Math.
Software 24, 4, 437*474. Also appeared as Max—Planck Institut fur Meteorologie Hamburg,
Technical Report No. 212, 1996.

GOEDECKER, S. AND HOISIE, A. 2001. Performance Optimisation of Numerically Intensive Codes.
Software, Environments, Tools. SIAM, Philadelphia. ISBN 0—89871—482—3.

GRIEWANK, A. 2000. Evaluating Derivatives: Principles and Techniques of Algorithmic Diﬁer-
entiation. Number 19 in Frontiers in Appl. Math. SIAM, Philadelphia, Penn.

GRIEWANK, A. AND CORLISS, G. F., Eds. 1991. Automatic Diﬁerentiation of Algorithms: Theory,
Implementation, and Application. SIAM, Philadelphia, Penn.

GRIEWANK, A., JUEDES, D., AND UTKE, J. 1996. ADOL*C, a package for the automatic differen—
tiation of algorithms written in C/C++. ACM Trans. Math. Software 22, 2, 131*167.

GRIEWANK, A. AND REESE, S. 1991. On the calculation of Jacobian matrices by the Markowitz
rule. See Griewank and Corliss [1991], 126*135.

HASCOET, L., NAUMANN, U., AND PASCUAL, V. 2003. TBR analysis in reverse mode automatic
differentiation. In Future Generation Computer Systems. Elsevier.

HOVLAND, P. D. AND MCINNES, L. C. 2001. Parallel simulation of compressible ﬂow using auto—
matic differentiation and PETSc. Parallel Computing 27, 4 (March), 503*519.

KNUTH, D. E. 1997. The Art of Computer Programming, Volume 1: Fundamental Algorithms.
Adison—Wesley.

MARKOWITZ, H. 1957. The elimination form of the inverse and its application. Management
Science 3, 257*269.

NAUMANN, U. 1999. Efﬁcient calculation of Jacobian matrices by optimized application of the
chain rule to computational graphs. Ph.D. thesis, Technical University of Dresden.

NAUMANN, U. 2001. Elimination techniques for cheap Jacobians. See Corliss et al. [2001], Chap—
ter 29, 241*246.

NAUMANN, U. 2002a. Elimination methods in computational graphs. on the optimal accumulation
of Jacobian matrices— conceptual framework. Mathematical Programming. (Submitted).

NAUMANN, U. 2002b. On optimal Jacobian accumulation for single expression use programs.
Mathematical Programming. (Submitted).

PARR, T., LILLY, J., WELLS, P., KLAREN, R., ILLOUz, M., MITCHELL, J., STANCHFIELD, S., COKER,
J., ZUKOWSKI, M., AND FLACK, C. 2000. ANTLR Reference Manual. Tech. rep., MageLang
Institute’s jGuru.com. January. See www.antlr. org/doc/index.html.

PRYCE, J. D. AND REID, J. K. 1998. ADOl, a Fortran 90 code for automatic differentiation.
Tech. Rep. RAL—TR—1998—057, Rutherford Appleton Laboratory, Chilton, Didcot, Oxfordshire,
OX11 OQX, England. See ftp://matisa.cc.rl.ac.uk/pub/reports/erAL98057.ps.gz.

REID, J. 2003. On the stability of automatic differentiation. To appear.

ROE, P. L. 1981. Approximate Riemann solvers, parameter vectors, and difference schemes.
Journal of Computational Physics 43, 357*372.

TADJOUDDINE, M. 1999. La differentiation automatique. Ph.D. thesis, Université de Nice, Sophia
Antipolis, France.

TADJOUDDINE, M., EYSETTE, E, AND FAURE, C. 1998. Sparse Jacobean computation in automatic
differentiation by static program analysis. In Proceedings of the Fifth International Static
Analysis Symposium. Number 1503 in Lecture Notes in Computer Science. Springer—Verlag,
Pisa, Italy, 311*326.

TADJOUDDINE, M., FORTH, S. A., AND PRYCE, J. D. 2001. AD tools and prospects for optimal
AD in CFD ﬂux Jacobian calculations. See Corliss et al. [2001], Chapter 30, 247*252.

TADJOUDDINE, M., FORTH, S. A., AND PRYCE, J. D. 2003. Hierarchical automatic differentiation
by vertex elimination and source transformation. In Computational Science and its Applications
- ICCSA 2003. Lecture Notes in Computer Science, vol. 2. Springer—Verlag, Montreal, 115*124.

TADJOUDDINE, M., FORTH, S. A., PRYCE, J. D., AND REID, J. K. 2002. Performance issues
for vertex elimination methods in computing Jacobians using automatic differentiation. In
Proceedings of the Second International Conference on Computational Science, P. M. Sloot,
Ed. Lecture Notes in Computer Science, vol. 2. Springer—Verlag, Amsterdam, 1077*1086.

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

October 11, 2004 Jacobian Code by Vertex Elimination ' 299

VERMA, A. 1998. ADMAT: Automatic differentiation in MATLAB using object oriented methods.
In SIAM Interdisciplinary Workshop on Object Oriented Methods for Interoperability. SIAM,
National Science Foundation, Yorktown Heights, New York, 174*183.

WILKINSON, J. 1965. The Algebraic Eigenvalue Problem. Oxford University Press.

Received Dec 2002; revised October 2003; revised February 2004; accepted March 2004.

ACM Transactions on Mathematical Software, Vol. 30, NO. 3, Sep. 2004.

