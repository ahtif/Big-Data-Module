Performance Issues for Vertex Elimination
Methods in Computing Jacobians using
Automatic Differentiation

Mohamed Tadjouddinel, Shaun A. Forthl, John D. Pryce2, and John K. Reid3

1 Applied Mathematics & Operational Research, ESD
Cranﬁeld University (RMCS Shrivenham), Swindon SN6 8LA, UK
{M.Tadjouddine, S.A.Forth}@rmcs.cranfield.ac.u.k
2 Computer Information Systems Engineering, DOIS
Cranﬁeld University (RMCS Shrivenham), Swindon SN6 8LA, UK
J .D.Pryce@rmcs . cranfield.ac .uk
3 JKR Associates, 24 Oxford Road, Benson, Oxon OX10 6LX, UK
jkr0r1.ac .uk

Abstract. In this paper, we present ﬁrst results from EliAD, a new
automatic differentiation tool. EliAD uses the Elimination approach for
Automatic Differentation ﬁrst advocated by Griewank and Reese [Auto-
matic Diﬁerentiation of Algorithms, SIAM (1991), 1267135]. EliAD im—
plements this technique via source—transformation, writing new Fortran
code for the J acobians of functions deﬁned by existing Fortran code. Our
results are from applying EliAD to the Roe ﬂux routine commonly used
in computational ﬂuid dynamics. We show that we can produce code
that calculates the associated ﬂux J acobian approaching or in excess of
twice the speed of current state—of—the—art automatic differentiation tools.
However, in order to do so we must take into account the architecture
on which we are running our code. In particular, on processors that do
not support out-of-order execution, we must reorder our derivative code
so that values may be reused while in arithmetic registers in order that
the ﬂoating point arithmetic pipeline may be kept full.

1 Introduction

In scientiﬁc computation, there is a frequent need to compute ﬁrst derivatives
of a function represented by a computer program. One way to achieve this is to
use Automatic Differentiation (AD) [1—3], which allows for the computation of
derivatives of a function represented by a computer program. The most eﬂicient
way to implement AD in terms of run-time speed is usually source transforma-
tion; here the original code is augmented by statements that calculate the needed
derivatives. ADIFOR [4], Odyssée [5], and TAMC [6] are well-established tools
for this which make use of the standard forward and reverse modes of AD.

We have developed a new AD tool EliAD [7,8] which also uses source trans-
formation. EliAD is written in Java and uses a parsing front-end generated by
the ANTLR tool [9]. In contrast to the AD tools listed above, EliAD uses the

vertex elimination approach of Griewank and Reese [2, 10], later generalised to
edge and face eliminations by Naumann [11, 12]. Here, we consider only the ver-
tex elimination approach, which typically needs less ﬂoating point operations
to calculate a Jacobian than the traditional forward and reverse methods im-
plemented by ADIFOR, Odyssée or TAMC. We introduce the vertex elimination
approach in Sect. 2.

In Sect. 3 we present results of applying our tool to the Roe ﬂux [13] com-
putation which is a central part of many computational ﬂuid dynamics codes.
We used various ways to sequence the elimination. We ran the resulting deriva-
tive codes on various machine/compiler combinations and at various levels of
compiler optimisation. We found that execution times were not always in pro-
portion to the number of ﬂoating point operations and that this effect was very
machine-dependent.

Section 4 discusses the performance of certain elimination strategies regard-
ing such machine dependent issues as cache and register utilisation and then
points out the importance of statement ordering on certain processors.

2 Automatic Differentiation by Elimination Techniques

AD relies on the use of the chain rule of calculus applied to elementary operations
in an automated fashion. The input variables a: with respect to which we need
to compute derivatives are called independent variables. The output variables y
whose derivatives are desired are called dependent variables. A variable which
depends on an independent variable, and on which a dependent variable depends,
is called an intermediate variable.

To illustrate the elimination approach, consider the code fragment comprising
four scalar assignments in the left of

$3 = ¢3(1‘1,$2) V153 = 03,1V151 + 03,2V152
1:4 = ¢4($2, $3) V254 = 043ng + 043ng (1)
$5 = ¢5(1‘1,$3) V155 = 05,1V1‘1 + 05,3V153

y = $411,355) V11 06,4V1‘4 + 06,5V155 -

Denoting

 

 

_ 617i 617i _ a¢i
V15, — (8151’6152) and cm — 825,- ,

we may use standard rules of calculus to write the linearised equations to the
right of (1). These linearised equations describe the forward mode of AD and
equivalent code would be produced by ADIFOR or TAMC enabling Jacobian
calculation on setting V151 = (1,0) and V152 = (0, 1). The Computational Graph
(CC) for derivative calculation is sketched in Fig. 1. Vertices 1, 2 represent the
independents x1, x2; vertices 3, 4, 5 the intermediate variables $3,254,155; and
vertex 6 the dependent y. The edges are labelled with the local derivatives of the
equivalent code statements given to the right of (1) as shown. The corresponding

6

O
5 / \ 4
01W ﬁgs 0

on

Fig. 1. An example computational graph

matrix representation gives a linear system, to solve for the derivatives, with zero
entries in the matrix omitted for clarity:

—1 V251 —1 0
—1 V172 0 —1

63,1 63,2 —1 V173 _ 0 0
64,2 64,3 —1 V174 — 0 0
65,1 65,3 —1 V175 0 0
66,4 66,5 —1 Vy 0 0

We may interpret the elimination approach via either the computational
graph or the matrix representation. Via the graph the Jacobian 3(be is de-
termined by eliminating intermediate vertices from the graph until it becomes
bipartite, see [2,10,11]. For example vertex 5 may be eliminated by creating
new edges 06,1 = 065 x 05,1, 063 = 065 x 053 and then deleting vertex 5 and all
the adjacent edges. We might then eliminate vertex 4 then 3, termed a reverse
ordering.

In terms of the matrix representation, vertex elimination is equivalent to
choosing a diagonal pivot from rows 3 to 5, in some order. At each step we
eliminate all the coeﬂicients under that pivot. For example by choosing row 5 as
the pivot row we add the multiple 06,5 of row 5 to row 6 and so produce entries
06,1 = 065 x 05,1, 063 = 065 x 053 in row 6. We see that this is identical to the
above computational graph description. By then choosing rows 4 and 3 as pivots
we are left with the Jacobian in elements 061 and 062.

There are as many vertex elimination sequences as there are permutations
of the intermediate vertices. We choose a sequence using heuristics from sparse
matrix technology aimed at reducing ﬁll-in, such as the Markowitz criterion
studied in [10, 11]. By reducing ﬁll-in such techniques have the desired side-effect
of choosing an ordering which minimises the number of ﬂoating point operations
at each elimination step.

3 Numerical Results

A typical application is the Roe ﬂux routine [13]. This computes the numeri-
cal ﬂuxes of mass, energy and momentum across a cell face in a ﬁnite-volume
compressible ﬂow calculation. Roe’s ﬂux takes as input 2 vectors of length 5
describing the ﬂow either side of a cell face and returns as output a length 5
vector for the numerical ﬂux (139 lines of code). We seek the 5 x 10 Jacobian
matrix.

We used EliAD to differentiate the Roe ﬂux code with different vertex elim-
ination orderings. Those orderings are Forward, Reverse, Markowitz (Mark),
VLR, Markowitz Reverse Bias (Mark RB), and VLR Reverse Bias (VLR RB)
eliminations; see for instance [11, 14] for details. The reverse-bias variants resolve
ties when there are several nodes of the same cost by taking the one that appears
last in the original statement order [14].

From the input code we generated derivative codes in one of the two following
manners.

1. Statement level: local derivatives are computed for each statement, no matter
how many variables 1:,- appear in its right-hand side.

2. Code list: local derivatives are computed for each statement in a rewritten
code that has at most two variables x,- in each right-hand side.

Then, we applied the different vertex elimination strategies to the resulting com-
putational graphs (62 intermediate vertices for the ﬁrst case and 208 interme-
diates for the second case). We ran the subsequent derivative codes on the fol-
lowing platforms: Silicon Graphics (SGI), Compaq Alpha (ALP), and two SUN
machines with different compilers denoted by SUN, NAG and FUJ. The various
processor/compiler combinations are described in Table 1.

Table 1. Platforms (processors and compilers)

 

 

Label Processor CPU L1—Cache L2—Cache Compiler Options
SGI R12000 300MHz 64KB 8MB f90 MIPSPro 7.3 iOfast
ALP EV6 667MHz 128KB 8MB Compaq f95 5.4 705
SUN Ultra10 440MHz 32KB 2MB Workshop f90 6.0 ifast
NAG Ultra10 440MHz 32KB 2MB Nagware f95 4.0a 703 inative
FUJ Ultral 143MHz 32KB 0.5MB Fujitsu f90 5.0 *Kfast

 

Table 2 shows the CPU times required by the calculation of the original
function on the different platforms.

Table 3 summarises the ratio between the timings of the Jacobian and the
original function by using the different methods and platforms. Methods starting
with VE are those using a vertex elimination strategy from our AD tool. We write
FD for one-sided ﬁnite differences, SL for statement level and CL for code list.
For TAMC, the postﬁx ftl denotes the forward mode and ad denotes the reverse
mode.

Table 2. Roe ﬂux CPU timings (in p seconds) of the function, T(F), on different
platforms

SGI ALP SUN NAG FUJ
0.8 0.5 0.9 1.8 6.2

Timings are based on 10, 000 evaluations for each method and an average
of 10 runs of the executables from the different machines we used. We have
checked the accuracy by evaluating the largest difference from the corresponding
derivative found by ADIFOR [4]. This was less than 10‘14 in all cases except ﬁnite
differences, where it was about 10‘7, in line with the truncation error associated
with this approximation.

Table 3. Ratios of Jacobian to Function CPU timings, T (VF) /T(F), on various plat—
forms, ratios of ﬂoating point operations, rﬂops = FLOPs(VF)/FLOPs(F), and num—
bers of lines of code (#106)

 

 

No. Method SGI ALP SUN NAG FUJ mop, #loc
1 FD (1—sided) 12.1 12.6 12.6 13.3 11.9 11.4 176
2 VE Forward(SL) 7.5 5.5 13.2 12.2 10.2 8.4 1534
3 VE Reverse(SL) 6.5 4.6 8.8 8.8 6.9 6.9 1318
4 VE Mark(SL) 6.4 5.1 9.8 10.4 6.7 7.2 1267
5 VE Mark RB(SL) 6.6 5.0 14.1 10.2 6.6 7.2 1261
6 VE VLR(SL) 6.0 4.5 8.9 9.1 6.1 6.5 1172
7 VE VLRRB(SL) 6.2 4.6 9.0 9.6 6.0 6.5 1170
8 VEForward(CL) 7.3 5.2 18.7 19.2 14.9 12.1 3175

9 VEReverse(CL) 6.7 4.7 10.7 9.9 9.5 8.9 2433

10 VE Mark(CL) 6.6 5.0 9.3 11.7 11.2 7.9 2058
11 VE Mark RB(CL) 6.6 5.3 10.2 11.0 10.9 7.7 2012
12 VE VLR(CL) 6.4 5.0 10.1 11.1 10.9 7.8 2116
13 VE VLR RB(CL) 7.2 5.4 10.8 12.4 12.1 8.4 2145
14 ADIFOR 15.9 9.8 31.4 50.5 14.4 15.0 614
15 TAMC—ftl 14.4 10.2 11.9 56.7 14.9 19.9 639
16 TAMC—ad 12.2 8.5 13.2 42.0 10.0 11.9 919

 

Table 3 shows that on the SGI and ALP platforms the vertex elimination
approach for computing J acobians is about twice as fast as both conventional AD
tools and ﬁnite differences. On the SUN, NAG and FUJ platforms the vertex
elimination approach does not reach the same level of eﬂiciency. We observe
that derivative codes for the code list version are often slightly slower than their
counterparts differentiated at statement level for ALP and SGI and sometimes
much slower on SUN, NAG and FUJ. Denoting by FLOPs(F) and FLOPs(VF)
the numbers of ﬂoating point operations required to compute respectively the

function and its Jacobian, we obtained the ratios also shown in Table 3 from
the Roe ﬂux test case. We see that on the SGI and ALP platforms the vertex
elimination derivative code ran faster than the ratio of ﬂoating point operations
predicts. On the SUN, NAG and FUJ platforms the reverse is usually true.

4 Performance Issues

The elimination strategies we used are based on criteria aimed at minimising the
number of ﬂoating point multiplications required to accumulate the Jacobian.
Van der Wijngaar and Saphir [15] have shown that, on RISC platforms, neither
the number of ﬂoating point operations nor even the cache miss rate are suﬂicient
to explain performance of a numerical code; and processor-speciﬁc instructions
(which can be seen from the generated assembly code) and compiler maturity
affect performance.

4.1 Floating Point Performance

If we consider the derivative code speed in terms of ﬂoating point operations
per clock cycle as shown in Fig. 2, all the elimination strategies are performing
over 1 ﬂoating point operation per cycle on the SGI and ALP platforms. The
Compaq EV6 and R12000 can perform up to 2 ﬂoating point operations per
clock cycle [16] and a throughput of in excess of one ﬂoating point operation
per clock cycle may be considered highly satisfactory. On the SUN and NAG
platforms a throughput of less than half a ﬂoating point operation per clock
cycle is achieved. Again the theoretical maximum is 2. It was initially thought
that cache misses may explain the poor timing ratios on these platforms.

 

 

 

 

Fig.2. Number of ﬂoating point operations per cycle performed by FD (number 1)
and the elimination strategies (numbered 2713 as they appear in Table 3); the bars
represent (from left to right) results obtained from SGI, ALP, SUN, and NAG

4.2 Cache Misses

On the SGI, the outputs from SGI Origin’s SpeedShop proﬁler showed that the
derivative codes ﬁt in the instruction cache and they differed from each other
only by few cache misses in the primary/secondary data caches. The Compaq
Alpha EV6 processor we used has caches comparable to those of the MIPS
R12000 (see Table 1). Therefore, we do not expect cache misses to be crucial for
the SGI and ALP platforms.

The Sparc Ultra 10 has relatively small caches compared with the R12000 and
EV6. On the SUN and NAG platforms we noticed a performance degradation
mainly of the code list method. Here, the forward elimination from the code
list showed particularly poor performance. Proﬁling with the Sun Workshop 6.2
tools, we observed a 28% instruction cache miss rate for this forward elimination
strategy. Using a SUN Blade 1000 (600 MHz) with double the primary cache size,
that percentage came down to 10%. Interestingly the timing ratios remained
similar to those from the Ultra 10 indicating that although instruction cache
misses were occurring, the root cause of the poor performance lay elsewhere.

4.3 Statement Ordering in the Derivative Code

The SGI and ALP processors may perform 2 ﬂoating point operations per clock
cycle through their ﬂoating point pipeline, provided that the ﬂoating point oper-
ation being performed uses data currently in a register and does not require any
data from the output of an operation ahead of it and still in the pipeline. The
SGI platform has a latency of 2 clock cycles before the result of an operation
is available whereas for the ALP platform this is 4 clock cycles [16]. This may
explain the slightly better results from the SGI compared to the ALP in Fig. 2.

Apart from their larger cache sizes the other main difference between the
SGI / ALP platforms and the SUN / NAG / FUJ platforms is that the SGI and ALP
processors support out-of-order execution [16]. This technique involves maintain-
ing queues of ﬂoating point operations ready to be performed and if the one at
the head of the current queue requires a value currently being processed in the
ﬂoating point pipeline or not currently in a register, then the processor switches
to another queue. Use of this technique reduces the importance of instruction
scheduling by the compiler.

On the Ultra10 and Ultral processors of the SUN, NAG and FUJ platforms
there is no out-of-order execution. The optimising compiler must perform more
intensive optimisation of instruction scheduling in order to maintain good use
of the ﬂoating point pipeline. The derivative codes we produce are large (see
ﬁnal column of Table 3), contain no loops or branching and hence comprise
one large basic block. We therefore expected optimising compilers to schedule
ﬂoating point instructions effectively since there are no complications regarding
the control ﬂow of the program. Since this is obviously not occurring for the
SUN, NAG and FUJ platforms, we conjectured that the local optimisation of
instruction scheduling performed by the compiler might not be able to maintain
a good throughput in the ﬂoating point pipeline since statements using the

same variables might be separated by several hundred lines of source code. Since
an elimination sequence produces a certain set of elimination statements that
may be placed in any order that respects their dependencies, we additionally
conjectured that we might be able to reorder the statements in the derivative
code to make better use of variables currently in registers.

To assess the impact of statement ordering in the derivative code, we per-
turbed the order of statements without altering the data dependencies within
the code. We reordered the assignment statements with the aim of using each
assigned value soon after its assignment. We therefore used a modiﬁed version of
the depth ﬁrst search algorithm [17]. Namely, we regarded the statements in the
derivative code as the vertices of an acyclic graph, with the output statements
at the top and an edge from s to t if statement If uses the output from state-
ment 8. Then, we arranged the statements in the order produced by a depth-ﬁrst
traversal of this graph. The results are shown in Table 4, which also displays the
Table 3 results in brackets. We can see that statement reordering has greatly
improved many of the SUN, NAG and FUJ times, has improved most of the SGI
times, but has made no signiﬁcant difference to the ALP times. For instance,
looking at the SUN column of Table 4, the statement reordering has improved
the generated code using the forward elimination from the code list by 56%.
In this case, we observed that the resulting reordered derivative has 34% less
loads and 72% less stores than the original derivative code. This indicates that
the compiler was not achieving eﬂicient register usage for the original derivative
code. We now see that the Jacobian to function CPU time ratios have all signif-
icantly improved and we are approaching twice the eﬂiciency of the best of the
conventional techniques, AD or ﬁnite-differencing, of Table 3.

Table 4. Ratios, T (VF) /T(F), of the reordered derivative codes from the Roe ﬂux
test case on different platforms, timings in brackets are those before the reordering (c.f.
Table 3)

 

Method SGI ALP SUN NAG FUJ

VE Forward(SL) 6.7 (7.5) 5.4 (5.5) 9.3 (13.2) 7.8 (12.2) 7.4 (10.2)
VE Reverse(SL) 5.2 (6.5) 4.6 (4.6) 7.7 (8.8) 7.2 (8.8) 6.2 (6.9)

 

VE Mark(SL) 5.8 (6.4) 5.2 (5.1) 8.6 (9.8) 7.3 (10.4) 6.1 (6.7)
VE Mark RB(SL) 5.8 (6.6) 5.1 (5.0) 7.9 (14.1) 6.8 (10.2) 6.0 (6.6)
VE VLR(SL) 6.4 (6.0) 4.6 (4.5) 8.2 (8.9) 6.3 (9.1) 5.6 (6.1)

VE VLRRB(SL) 6.8 (6.2) 4.5 (4.6) 7.5 (9.0) 6.2 (9.6) 5.6 (6.0)
VE Forward(CL) 6.2 (7.3) 5.2 (5.2) 8.2 (18.7) 13.3 (19.2) 8.5 (14.9)
VE Reverse(CL) 6.4 (6.7) 5.0 (4.7) 8.8 (10.7) 7.5 (9.9) 7.4 (9.5)
VE Mark(CL) 6.6 (6.6) 5.1 (5.0) 7.8 (9.3) 7.3 (11.7) 6.1 (11.2)
VE Mark RB(CL) 6.3 (6.6) 5.4 (5.3) 8.6 (10.2) 7.5 (11.0) 6.1 (10.9)
VE VLR(CL) 7.1 (6.4) 4.9 (5.0) 7.5 (10.1) 6.8 (11.1) 6.2 (10.9)
VE VLRRB(CL) 6.8 (7.2) 5.8 (5.4) 8.1 (10.8) 8.1 (12.4) 6.4 (12.1)

 

5 Conclusions

Automatic differentiation via vertex elimination allows us to produce Jacobian
code which, in conjunction with an optimising compiler, eﬂiciently uses a ﬂoat-
ing point pipeline on processors that support out-of-order execution. On other
platforms tested and that supported a ﬂoating point pipeline we found that the
compiler was not able to schedule the ﬂoating point operations with enough
register re-use to allow the ﬂoating point processor to perform eﬂiciently. Appli-
cation of a simple statement reordering strategy based on depth ﬁrst traversal
enabled the compiler to optimise the resulting derivative code more effectively.

We conclude that we have an AD tool that, when assisted by statement
reordering on some platforms, produces Jacobian code approaching or exceeding
twice the eﬂiciency of the current state-of-the-art for the Roe ﬂux test problem.
We are currently testing and evaluating the tool’s performance on a number of
test problems taken from the MINPACK-2 optimisation test suite [18].

Acknowledgments

We thank EPSRC and UK MOD for funding this project under grant GR/R21882.

References

1. Griewank, A., Corliss, G.: Automatic Differentiation of Algorithms. SIAM,
Philadelphia (1991)

2. Griewank, A.: Evaluating Derivatives: Principles and Techniques of Algorithmic
Differentiation. SIAM, Philadelphia (2000)

3. Corliss, G., Faure, C., Griewank, A., Hascoét, L., Naumann, U., eds.: Automatic
Differentiation of Algorithms: From Simulation to Optimization. Springer (2002)
(to appear).

4. Bischof, C., Carle, A., Khademi, P., Mauer, A.: ADIFOR 2.0: Automatic differ—
entiation of Fortran 77 programs. IEEE Computational Science & Engineering 3
(1996) 18732

5. Faure, C., Papegay, Y.: Odyssée user’s guide, version 1.7. Technical Re—
port 0224, INRIA, Unité de Recherche, INRIA, Sophia Antipolis, 2004 Route
des Lucioles, BR 93, 06902, Sophia Antipolis Cedex, Fiance (1998) See
http://ww.inria.fr/safir/SAM/Odyssee/odyssee.html.

6. Giering, R., Kaminski, T.: Recipes for adjoint code construction. ACM Transac—
tions on Mathematical Software 24 (1998) 4377474

7. Tadjouddine, M., Forth, S.A., Pryce, J.D., Reid, J.K.: On the Implementation of
AD using Elimination Methods via Source Transformation: Derivative code gen—
eration. AMOR Report 01/4, Cranﬁeld University (RMCS Shrivenham), Swindon
SN6 8LA, England (2001)

8. Tadjouddine, M., Pryce, J.D., Forth, S.A.z On the Implementation of AD using
Elimination Methods via Source Transformation. AMOR Report 00/8, Cranﬁeld
University (RMCS Shrivenham), Swindon SN6 8LA, England (2000)

9. Parr, T., Lilly, J., Wells, P., Klaren, R., Illouz, M., Mitchell, J., Stanchﬁeld, S.,
Coker, J ., Zukowski, M., Flack, C.: ANTLR Reference Manual. Technical report,
MageLang Institute’s jGuru.com (2000) See http://ww.a.ntlr.org/doc/.

10.

11.

12.

13.

14.

15.

16.

17.

18.

Griewank, A., Reese, 8.: On the calculation of Jacobian matrices by the Markowitz
rule. [1] 1267135

Naumann, U.: Efﬁcient Calculation of J acobian Matrices by Optimized Application
of the Chain Rule to Computational Graphs. PhD thesis, Technical University of
Dresden (1999)

Naumann, U.: Elimination techniques for cheap J acobians. [3] 2417246

Roe, P.L.: Approximate Riemann solvers, parameter vectors, and difference
schemes. Journal of Computational Physics 43 (1981) 3577372

Tadjouddine, M., Forth, S.A., Pryce, J.D.z AD tools and prospects for optimal AD
in CFD ﬂux Jacobian calculations. [3] 2477252

der Wijngaart, R.F.V., Saphir, W.C.: On the efficacy of source code optimizations
for cache—based processors. NAS Technical Report NA87007014, NASA (2000)
Goedecker, S., Hoisie, A.: Performance Optimization of Numerically Intensive
Codes. SIAM Philadelphia (2001)

Knuth, D.E.: The Art of Computer Programming, Volume 1: Fundamental Algo—
rithms. Adison—Wesley (1997)

Averick, B.M., More, J .J .2 User guide for the MINPACK—2 test problem collection.
Technical Memorandum ANL/MCSeTM7157, Argonne National Laboratory, Ar—
gonne, Ill. (1991) Also issued as Preprint 917101 of the Army High Performance
Computing Research Center at the University of Minnesota.

