FAST AD JACOBIANS BY COMPACT LU FACTORIZATION

JOHN D, PRYCE" AND EMMANUEL M, TADJOUDDINET

Abstract. For a vector function, coded without branches or loops, code for the Jacobian
is generated by interpreting Griewank and Reese’s Vertex Elimination as Gaussian elimination, and
implementing this as compact LU factorization, Tests on several platforms show such code is typically
4 to 20 times faster than that produced by tools such as ADIFOR, TAMC or TAPENADE, on average
signiﬁcantly faster than Vertex Elimination code produced by the ELIAD tool (Tadjouddine et al. in
LNCS vol, 2330, 2002) and can outperform a hand—coded Jacobian,

The LU approach is promising, e.g., for CFD ﬁux functions that are central to assembling Ja—
cobians in ﬁnite element or ﬁnite volume calculations, and in general for any inner—loop basic block
whose Jacobian is crucial to an overall computation involving derivatives,

Key words. Automatic Differentiation, Vertex Elimination, Source Transformation, Abstract
Computational Graph, Sparse Matrix, LU Factorization,

AMS subject classiﬁcations. 65K05, 65Y20, 65F50, 15A24, 90C90

1. Introduction. Automatic Differentiation (AD) is now a standard technology
for computing derivatives of a (vector) function f(x) deﬁned by computer code, such
as sensitivities with respect to design parameters, Jacobians for use in Newton and
related iterations, Taylor series generation, etc. This article is concerned with ﬁrst
derivatives, that is with the Jacobian matrix J : J(X) : f’(x) of f.

Especially for large industrial problems, the preferred approach is source—to—sourcc
translation, where the code for f is pre—processed to produce Jacobian code, that is
code for f’ . With current compilers, such code generally runs an order of magnitude
faster than the alternative of generating derivatives by operator overloading.

For functions deﬁned by straight—line code, this paper shows the vcrtcm elimination
(VE) method of Griewank and Reese [9] is a form of Gaussian elimination, and sets it
in the framework of sparse matrix factorization as described, e.g., in Duff, Erisman and
Reid [5]. In the experiments presented here we have implemented it as a compact LU
factorization (usually named after Crout or Doolittle). Standard VE implementations,
e.g. [6], produce code with many short statements, while the LU style produces fewer,
longer statements of inner—product form. Each affected entry in the extended J acobian
is changed just once, with a statement cij : cij + ZkeK cikckj if updating an original
local derivative, or cij : ZkeKCikaj if creating ﬁll—in. The structure of LU—style
Jacobian code makes it easy to do certain optimizations locally that standard VE
cannot do without a more global code analysis.

In the graph viewpoint, an entry cij labels the edge from vertexj to vertex i, K is
the set of vertices k such that there is a 2—edge pathj ~> k ~> i, and each affected edge
j ~> i is updated/created by collapsing such paths into it. Researchers into vertex
elimination AD have mainly thought in terms of manipulating the computational
graph (Griewank [8], Naumann [11]). They acknowledge the linear algebra but see it
as secondary. Our approach is hard to devise without taking a linear algebra View.

As with other sparse factorizations, the pivot order is important and is chosen
with the aim of controlling ﬁll—in. Following Naumann’s work on VE, e.g. [12], we

*ClSE, Department Of Informatics and Simulation, Cranﬁeld University (RMCS Shrivenham),
Swindon SN1 8LA, UK

1LDepartment of Computing Science, University of Aberdeen, King’s College, Aberdeen AB24
3UA, UK

2 John D, Pryce and Emmanuel M, Tadjouddine

have used in our tests the classic Markowitz local algorithm; Naumann’s VLR variant
of Markowitz; simple forward and reverse ordering; and “pre—elimination” versions of
these where all intermediate vertices in the CG that have a single successor are used
ﬁrst. Pre—elimination was recommended by Naumann in [12], and in earlier tests of
VE with the ELIAD tool [16, 17] it gave promising results.

The functions that originally interested us in this work were ﬂux “kernels” for
ﬁnite element or ﬁnite volume calculations in CFD. These are at most a few hundred
lines of code, and typically a resulting Jacobian routine will be called hundreds of
thousands of times, or more, in a CFD calculation while assembling a large sparse
Jacobian from pieces. In such applications, algorithms for choosing the pivot order
are a negligible part of the overall cost and we have not paid any attention to their
efﬁciency; our MarkowitZ—style methods are slow for large systems. From the sparse
matrix mainstream, work such as Amestoy, Li and Ng [1], and references therein,
presents asymptotically faster MarkowitZ—like algorithms that would be useful if our
present work proves relevant to J acobians of lengthy functions.

Our performance tests show that code produced by LU methods usually ran faster
than other VE—style code produced by the ELIAD tool; it was generally comparable
with hand coding and much faster than code from other tools such as TAPENADE.

The material is structured as follows. Section 2 describes the standard approach
to Vertex Elimination, and the linear algebra viewpoint leading to the LU approach.
Section 3 outlines our code generation method. Section 4 describes our performance
comparisons between Jacobian codes produced by various methods for a number of
functions and a number of platforms. Section 5 draws conclusions.

2. Vertex and edge elimination.

2.1. Code lists. The calculation of f will be described by a code list [8], equiv—
alent to static single—assignment form [4]. It is a sequence of equations

v,- : go,- (relevant previous vj), (2.1)

for i : 1, . . . , p + m. The go,- are given elementary functions and “relevant previous
vj” denotes those variables vj that are the actual arguments of go,- 7 necessarily
all having j < i. Here, v1,n,...,v0 are aliases for f’s input variables $1,...,xn,
while vp+1, . . . , vp+m are aliases for f’s output variables yl, . . . ,ym, and v1, . . . ,vp are
intermediate variables. That is, there are n inputs, p intermediates and m outputs.

A code list describes the values calculated by a single execution—trace through the
program code of f. This paper does not study how Jacobian code for functions, whose
code contains branches and loops, may be generated. For standard vertex—elimination
this has been implemented in ELIAD, see [16].

2.2. A simple AD example. The sparse linear algebra View of computing a
Jacobian by AD will be illustrated by a simple function y : f(x) with three inputs
and two outputs. The left column of the table below shows a code list for f, written
in MATLAB—like notation. On the right the code list variables, written in normal

AD Jacobians by compact LU 3

mathematical notation, are shown as functions of the inputs $1, $2, $3.

function [y1,y2] = f(x1,x2,x3)

v1 = x1*x2 711 : 051052

v2 = sin(v1) v2 = sin($1$2)

v3 = 2*v2 v3 = 2sin($1$2) (2.2)
v4 = V3-x1 v4 = 2sin($1$2) — $1

yl = x3*v4 y1 = $3 (2 sin($1$2) — $1)

y2 = 3*v4 y2 = 3 (2 sin($1$2) — $1)

The inputs are x1,x2,x3, the dependents are 311,312 and the intermediates are
v1,v2,v3,v4. We wish to generate code to calculate J : 8(y1,y2)/8($1,$2,$3),
comprising 83/1/8531, din/[M2, etc.

The basic linear relations of AD are obtained by differentiating line—by—line:

v1 = x1*x2 dvl = $2 d$1 + $1 d$2

v2 = sin(v1) dv2 = cos(v1)dv1

v3 = 2*v2 (1713 = 2dv2 ‘
v4 = v3-x1 dv4 = (1713 — d$1 (2'3)
yl = x3*v4 dyl = $3 dv4 + 714 d$3

y2 = 3*v4 dy2 = 3dv4

The d’s mean “derivatives with respect to whatever parameters we are interested in”.
Eliminating intermediate dvk to get the dyi as linear combinations of the d$j,

dyi : Z Jij d$j,
1'

one obtains J : [Jij], the desired Jacobian matrix.

One way of computing J is by classical forward AD. Here, d means gradient
with respect to the input variables. In our example, d : (8/8$1, 8/8$2,8/8$3). The
process is shown in the table below.

 

Initialize with
d$1 : (1 0 0)
d$2 : (0 1 0)
d$3 : (0 0 1)
and continue
dvl = $2 d$1 + $1 d$2 = ($2 $1 0)
dv2 = cos(v1)dv1 = (cos(v1) $2 cos(v1) $1 0) (2.4)
(1713 : 2 (1712 : . . .
(1714 : (1713 — (11E1 :
ending with
(1711 : $3 (1714 -]- 714 (11E3 : ...
(1712 : 3 (1714 : .

 

 

 

(formulae for entries in last four rows omitted)

This of course is done numerically at run time, not in the symbolic way suggested in
this table. The process amounts to eliminating the dvk by forward substitution.

4 John D, Pryce and Emmanuel M, Tadjouddine

2.3. A linear algebra viewpoint.
The equations in matri$ form. The relations in (2.3) form a sparse linear system

 

 

 

 

 

n 7 3 p : 4 m : 2 d$1
$2 $1 0 *1 0 0 0 0 0 d$2
p:4 0 0 0 cos(v1) 71 0 0 0 0 3Z3
1
0 0 0 0 2 *1 0 0 0 dv2 : 0 (2.5)
*1 0 0 0 0 1 *1 0 0 C123
m:2 [ 0 0 714 0 0 0 $3 71 0 J dm
0 0 0 0 0 0 3 0 71 dyl
C1112
The system illustrated by (2.5) may be written in general as
n p m dx
P [B L ’ I 0] dv : 0. (2.6)
m R T fl dy

This is part of the e$tended Jacobian [8, p. 22] of Griewank, namely CO 7 I where CO
is the square matrix

n p m
n 0 0 0

CO: [9 [B L 0].
m R T 0

The non—zeros of CO are the local derivatives cij of the code list. With rows and
columns indexed (17n), (27n), ..., (p+m) 7 to match the aliased names for the
input and output variables 7 we have

8901'
(97}j 7

 

cij : 131‘: p+m, 17n Sj S p+m, (2.7)
where go,- is regarded as a function of all the vj, taking cij as zero if vj is not one of
the variables on which go,- depends.

The bottom right block of the matrix is shown as il in (2.6). This is not so
when some output yk is used in the deﬁnition of another output 31,-. To eliminate such
cases, introduce an extra intermediate that is a copy of yk. We assume this has been
done. Then all required data are contained in the (p+m) X (n+p) matrix:

71 I?
C: P [B L]. (28)
m R T

where L : L 7 I. Thus B holds the (local) derivatives of the intermediates w.r.t.
the inputs, and so on. Following Co’s numbering, the rows are numbered 1, . . . ,p+m
and the columns 17n, . . . ,p, so the 71’s on the diagonal of L are in positions (16,16),
k : 1,. . . ,p. Submatrix L is~strictly lower triangular because each variable depends
only on previous ones. Thus L is non—singular, being triangular with nonzero diagonal.

AD Jacobians by compact LU 5

The Jacobian J is a function purely of the entries of C. Solving (2.6) we ﬁnd
dy : (R 7 TL’IB) dx, thus J is given by

J = 1(0) = R 7 TL’IB, (2.9)

the Schur complement of L in C. This was noted in Griewank [8].

Verte$— and edge—elimination. Griewank and Reese’s presentation of verte$—
elimination [9] and Naumann’s of forward and backward edge—elimination [11, 12] are
in terms of the computational graph. A key feature of them is that they transform
the graph, equivalently C, by successive steps that keep J(C) invariant. This can be
seen as “obvious”, because they transform the underlying linear equations in a way
that does not change the relation between the input and output variables. However
it seems useful to give a matrix proof of it, Lemma 2.2 below.

From the linear algebra viewpoint, vertex— and edge—elimination are sequences of
row or column operations on C as follows. The simple row operation on C whereby
an; times row k is added to row i, followed by multiplying row k by oak 7é 0, amounts
to pre—multiplying C by the (p+m) X (p+m) matrix that is the identity with its k—th
column replaced by ((11,. .. ,ozn) . A simple column operation on C post—multiplies
C by an (n+p) X (n+p) matrix with the transposed shape.

A forward [resp. backward] edge elimination step is such a row [column] operation,
that pivots in an intermediate row [column] with oak : 1 and only one other on; [resp.
ozj] nonzero, its value chosen to zero the (i, k) [resp. (k,j)] entry of C.

A verte$—elimination (VE) step can be described symmetrically by either a row or
a column operation. As a row operation it removes multiples of an intermediate row
k (k : 1, . . . ,p) of C from other rows, to zero all of column k below the pivot. It is a
standard step of Gaussian elimination (GE) where the update cij : cij 7 cikckj/ckk
can be simpliﬁed because always ckk : 71:

cij : cij 7 cikckj for i with cik 7é 0 and j with ckj 7é 0:
Cik : 0 for i > k: (2.10)
All other entries of C unchanged.

Regarded as a column operation, it does the same except that the second line reads:
ckj : 0 for j < k. The [C row and column are irrelevant to subsequent elimination
steps, and may be ignored 7 or deleted, which makes the row and column versions
have identical effects.

A complete verte$ elimination (CVE) consists of p steps (2.10), using all pivots
ckk : 71, (k : 1,. ..,p) in some order speciﬁed by the pivot order, a permutation
7r: (7r1,...,7rn) of 1,...,p.

To justify that CVE thus deﬁned is indeed equivalent to GE, whatever the pivot
order, we need to show that (i) no VE step alters any pivots from the value 71,
and (ii) the lower triangular structure is preserved. These follow because L is lower—
triangular, so line 1 of (2.10) only affects the block to the left of and below (16,16),
that is i > k and j < k. The foregoing implies:

PROPOSITION 2.1. CVE is equivalent to doing a symmetric permutation of the
intermediate rows and columns into pivot order (thus keeping the 71 ’s on the diagonal
of L) and then eliminating the intermediate variables by GE without interchanges.
Equivalence is in the sense that GE and CVE do identical arithmetic operations. In
inexact arithmetic, even the roundoff errors are the same.

6 John D, Pryce and Emmanuel M, Tadjouddine

2.4. An example elimination. For our example (2.2), the following traces
the effect of elimination in pivot order (3 2 1 4). Non—zeros with a known constant
value are shown by their value in a circle. Other non—zeros are marked 0, except
that a value just created is marked 2% if it updates an existing entry (thus requiring
one multiplication and one addition to compute) or X if it ﬁlls—in a previously zero
position (requiring one multiplication only). The about—to—be—used pivot is shown in
bold -1. The used pivot rows and columns are blanked out.

 

$1 $2 $3 711 712 713 714

v1 0 o 0 —1 0 0 0

~ v2 0 0 0 o —1 0 0

B L _ v3 0 0 0 0 ® —1 0
R T 7 v4 @ 0 0 0 0 (D —1
111 [0 0 . 0 0 0 .J

112 0 0 0 0 0 0 (33

Step 1: subtract multiples of 713 row from rows below
$1 $2 $3 711 712 713 7J4

 

711 o o 0 —1 0 0
v2 0 0 0 o —1 0
713

v4 @ 0 0 0 ® —1
yl [0 0 o 0 0 O]
112 0 0 0 ] 0 0 (23

Step 2: subtract multiples of 712 row from rows below
$1 $2 $3 711 712 713 714

 

v1 o o 0 —1 0
712
713
v4 @ 0 0 X —1
111 [0 0 o 0 a]
112 0 0 0 ] 0 (23

Step 3: subtract multiples of v1 row from rows below
$1 $2 $3 711 712 713 714

 

711

712

713

714 9K X 0 —1

111 [0 0 o o J
112 0 0 0 ] (33

$1 $2 $3 711 712 713 7J4

 

AD Jacobians by compact LU 7

At the end J occupies the R block. The above sequence requires 1 addition and 7
multiplications: compare 9 additions and 21 multiplications by forward AD if done as
in (2.4), treating the dvl- as full 3—vectors. This may seem an unfair comparison, but
even such a modern AD tool as TAPENADE uses full rather than sparse calculation in
its “vector” mode, which is its most natural way to compute Jacobians.

2.5. VE preserves the Jacobian. Any product of simple row [respectively,
column] operations that pivot in a row [column] belonging to an intermediate variable,
has a matrix of the form

19 m n p
U : I) W 0 , respectively V : n I 0 , (2.11)
m X I p Y Z

where W and Z are non—singular. Direct calculation veriﬁes the following, which is
actually a general property of Schur complements, see (2.9).

LEMMA 2.2.

(i). Given C in (2.8), the Jacobian J(C) is unchanged by simple row [column]
operations on C that pivot in an intermediate row [column], i.e. by any trans—
formation C : UCV with U and V as in (2.11).

(ii). Hence, any sequence of such operations that reduces TL’IB to zero, leaves
J(C) in the lower left block (the R block) of C.

A VE step, whether as a row or a column operation, satisﬁes the conditions of

Lemma 2.2. Any CVE reduces the T block to zero. Hence, by the Lemma, at the end
J occupies the R block of C.

2.6. The LU approach. In the graph View, a VB step amounts to creating,
or updating, edges from one of vertex k’s predecessors to one of its successors, and
then deleting vertex k. CVE is a sequence of atomic VE steps each of which, by the
Lemma, keeps J invariant. The LU approach cannot be interpreted as steps that keep
J invariant because the operations, though identical at the level of individual updates
cij : cij 7chkj, are in a different order. This may be why it has not been considered
by AD workers who are more comfortable with the graph View.

For what follows, we permute the intermediate variables to pivot order, swap the
ﬁrst and second column—blocks, and index the right—hand column block as is then
natural: p 71,...,p 7 n instead of 1 7 n, . . . ,0. This replaces C in (2.8) by

I? n I? n
.: .1: p (PLPT7I) PB : p t B 212
C (Cu) m TPT R m T R, say. (. )

Here P is the permutation matrix representing 7r. Matrix ﬂ is not lower—triangular
in general, but the pivots remain on its main diagonal: czk : 71 (k : 1, . . . ,p). The
Jacobian is still the Schur complement of the p X p block, J : R 7 TL’IB.

Since CVE is a partial GE, it is equivalent to an LU factorization. This equiv—
alence, and various approaches to LU for sparse matrices, are discussed in [5, Sec—
tion 3.577]. The discussion there is for square matrices but holds for the present case
too. In the following algorithm the “partial” behaviour, in the sense that elimination
is taken only as far as rows and columns 1 to p, is implemented by the upper bound
of the sum being min{i71,j71,p} instead of the usual min{i71,j71}.

8 John D, Pryce and Emmanuel M, Tadjouddine

ALGORITHM 2.1 (LU on 0*).
for i: 2,...,p7m, j : 2,...,p7n and i 7é j, in a suitable order
min{i71,j71,p}
set cf]. : cf]. 7 Z cfkc’gj
k:1
At the end, J is in the bottom right block of C*.

The ﬁrst row and column are unchanged. An order is “suitable” provided at each stage
those quantities required on the right hand side are already computed. Alternative
orders and their memory access patterns are described in [5, Section 3.7]. In our tests
we used a Crout order, which computes the (not already found) entries of the second
row and column, then of the third row and column, and so on.

The computational schemes we have tested are based on Algorithm 2.1. For
a given pivot order, the ﬂoating point operation counts (“op—counts”) of the CVE,
GE and LU methods are identical, given they exploit sparsity equally. When one
outperforms another it will be mainly due to code making better use of registers and
cache. LU also allows some useful optimizations at code—generation time.

3. Tools used in the experiments. This section describes the tool used to
generate Jacobian code for the experiments in Section 4, and also a tool for generating
“random functions”.

3.1. Jacobian code generation tool.

Summary. First, an example illustrates the kind of Jacobian code that is gener—
ated. The entries of C are represented by simple variables, rather than array elements.
Our convention is that, e.g., the (v13 row, $5 column) entry is the variable CV13X5.

A complete procedure to calculate J at a given input X consists of three parts.
The ﬁrst is the code of f , augmented by statements to compute the local derivatives
by AD. For instance a statement v17 = v15*x6 in the code list, i.e. v17 : v15$6, would
be augmented by cvl7v15 = x6 and cvl7x6 = v15 representing 8v17/8v15 : $6 and
dun/[Mg : v15. For statement—level mode (described in Subsection 4.1) the local
derivatives were produced by AD reverse mode on each individual statement.

The second part of J is the elimination code, which is the topic of this paper. The
ﬁnal part packs up the variables that comprise J. For instance in our simple example
J is the collection

J7 cylxl cy1x2 cy1x3
T cy2x1 cy2x2 O '

One must take care of structural zeros in J, such as J23 here.
We have to generate code for a sequence of assignments of the form

Cij = Cij + Z Cikaja (3-1)
kEK

where we only include terms cikckj for which both factors are (potentially) nonzero. In
AD applications, a large proportion of the original entries are constants, independent
of run—time problem data. For example, the local derivatives for $ i y are 1, i1; that
for art, where a is a constant, is a. If one propagates knowledge of constant cij values
as the elimination proceeds, the J acobian code can be much shortened.

GE—style elimination comprises assignments of the form cij : cikckj (ﬁll—in) or
cij : cij 7 cikckj (update), possibly altering a cij several times. Possible pseudo—code

AD Jacobians by compact LU 9

 

//Input: local derivatives
//cv1x1 , cv1x2 , cv2v1 , cv3v2 , cv4x1 ,

 

//cv4v3, cyle, cy1v4, cy2v4

//eliminate v3 //Input: non-constant elem derivs

// clel, cv1x2, cv2v1, cyle, cy1v4.

4 2 = 4 3* 3 2
CV V. . CV V CV V //Use cv3v2=2, cv4x1=—1, cv4v3=1, cy2v4=3
//elim1nate v2 cv4v1 _ 2*cv2v1
cv4v1 = cv4v2*cv2v1

cv4x1 = -1 + cv4v1*cv1x1
cv4x2 = cv4v1*cv1x2
cylxl = cy1v4*cv4x1
cy2x1 = 3*cv4x1

cy1x2 = cy1v4*cv4x2
cy2x2 = 3*cv4x2

//eliminate v1

cv4x1 = cv4x1 + cv4v1*cv1x1
cv4x2 = cv4v1*cv1x2
//eliminate v4

cylxl = cy1v4*cv4x1

 

 

 

cy2x1 = cy2v4*cv4x1
cy1x2 = cy1v4*cv4x2
cy2x2 = cy2v4*cv4x2

 

 

 

TABLE 31
Generation of Jacobian code for the simple e$ample. On the left: GE-style, not e$ploiting
known constants. On the right: LU—style, e$ploiting constants.

for the simple example (2.2), not exploiting constant values, is shown on the left side
of Table 3.1. It follows the steps shown graphically in Subsection 2.4.

LU style code generation for this example, with the entries generated in the
order suggested earlier, and exploiting constants, yields the code on the right side of
Table 3.1. The problem is too simple to show the “fewer, longer statements” feature.
Each assignment has the form (3.1) where cij on the right is omitted if it is currently
zero. K : K(i,j) is the set of indices k in the un—reordered C that correspond to
k : 1, . . . ,min{i 7 1,j 7 1,p} in the re—ordered matrix C* of (2.12).

Implementation outline. The LU—style Jacobian code (“Jcode”) generation tool
was written in MATLAB, which proved convenient because of its object—oriented pro—
gramming features, high level syntax and support for sparse matrices. The tool takes
as input (a) a ﬁle specifying the computational graph (CG) of the function f, (b) a
speciﬁcation of a heuristic (such as “Markowitz”) for choosing the pivot sequence and
(c) a speciﬁcation of the overall approach such as “LU” or “GE”. It produces the
same Jcode that a production tool might produce (only more slowly).

Separately, code to compute f’s local derivatives (“LDcode”) is generated. A
complete Jacobian subroutine is made of the LDcode followed by the Jcode, embedded
in template code of subroutine header and closing statements. For f given by pre—
existing Fortran code the CG ﬁle, LDcode and template code were generated by
processing f’s Fortran ﬁle with the ELIAD tool. For f generated by the “random
function” tool of Subsection 3.2, that tool produced the CG ﬁle, LDcode and template
code.

One can use a simple data structure that avoids the complexity of general com—
piler techniques. A combined numerical/symbolic elimination is done that can be
summarized:

Repeatedly, do (3.1) for a suitable sequence of triples i,j and K.

The sequence of i,j, K’s is determined during elimination by the overall elimination
strategy (pivot order: GE or LU style: Crout or some other order of operations) and

10 John D, Pryce and Emmanuel M, Tadjouddine

the resulting ﬁll—in. This is a version of the “loop—free code” approach to sparse matrix
factorization described in [5, Section 9.6].

The operation representing (3.1) acts on a “partially eliminated sparse matrix”
object that knows which of its entries are compile—time constants and which are run—
time values. Each call to the operation updates this matrix, and may output a
statement of J acobian code. The following scheme works well in MATLAB. Numerical
elimination is done on a sparse array C that initially holds the local derivatives cij in
(2.7). At each stage, cij either holds a ﬁnite numeric value, meaning that constant
value, or holds NaN, meaning a value unknown till run time. The rules of operations
on NaN, plus care to avoid the operation NaN X 0, imply this remains true throughout.

Code is generated for each occurrence of (3.1) that has least one NaN multiplied
by a nonzero value, doing obvious simpliﬁcations. For instance, suppose i : 12, j : 3,
K : {4,7,8, 11}, representing the assignment

:71 :71 :1 :1 :3,75
C12,3 :012,3 7 012,4 *04,3+ C12,7 * 07,3 7012,23 * C8,3 7 C12,11* C11,3

where known values are marked above the relevant cm. Suppose, say, indices 3,4,7,8,11
are intermediate variables, and 12 is output variable yl. Then when simpliﬁed using
the known values it gives the line of code

cy1v3 = -Cv4v3 + cy1v8*cv8v3 + cy1v11*3.75

If the right side simpliﬁes to the existing value of cij or to a constant, then (except
for an entry in the ﬁnal Jacobian) no code is generated.

Aliasing. Tests on our examples showed a large proportion of the statements
generated by this process were simple copies cij : cm or cij : 7cm. To eliminate
these, a table records equivalence classes of the cij under the relation cij : 0cm where
0' : i1. One member of the equivalence class is chosen as the “alias” of each other
member, and replaces it everywhere. If, in the above example, the table shows that
c873 has 7C473 as its alias, then the generated code changes to

cy1v3 = -Cv4v3 - cy1v8*cv4v3 + cy1v11*3.75

On the FIC problem reported below, aliasing removed over 500 statements to leave
just 244 statements 7 one per nonzero entry in J. For the random functions in our
experiments it typically removed over 20% of statements.

Comments. This implementation was intended for modest sized functions where
most or all of the variables used ﬁt within the cache of current machines. In particular
storing the Jacobian entries as simple variables gives no control over how they are
laid out in main memory.

When this works well, standard sparse matrix considerations of optimizing data
transfer between CPU and main memory are not very important. Below, we also test
what happens as the functions get “larger” and data transfer begins to dominate.

3.2. Random function tool. To enable rapid and extensive testing, we wrote
another MATLAB tool that allows us to generate a “random function” f by selecting
its numbers n, m and p of inputs, outputs and intermediate variables respectively, plus
a random number seed for reproducibility purposes. Each code list statement does
one arithmetic operation 7 7 X or 7, selected using the MATLAB random number
generator with probabilities reﬂecting typical numerical code. The tool outputs (a)
the CG for use by the other tool, (b) a Fortran ﬁle that computes just f, and (c) a
Fortran ﬁle that computes the local derivatives, into which code such as produced by
our tool is inserted to make a complete subroutine.

AD Jacobians by compact LU 11

a) Processors

 

 

Platform I Processor CPU speed L1—Cache L2—Cache
AMD AMD Opteron 2.2 GHz 32KB 1 MB
Intel Pentium D 2.99 GHz 32KB 2 MB
DELL Pentium M 1.7 GHz 64KB 512 KB
b) Compilers
Platform I OS Compiler Options
AMD G95 Linux/Centos G95 version 0.9 -03 -r8
Intel G95 Windows XP G95 version 0.9 -03 -r8
DELL G95 Windows XP G95—MinGW ver. 0.9 -03 -r8
DELL Salford Windows XP Salford FTN95 /optimise/p6/dreal
/silent/fpp
DELL Absoft Windows XP Absoft f95 Pro 10 -w -02 -N113
TABLE 41
Platforms

4. Experiments. We compared the speed of Jacobian code produced by the
LU approach with that produced by (a) other vertex elimination techniques, (b)
conventional AD tools ADIFOR, TAMC or TAPENADE, (c) hand—coded Jacobian where
available, (d) ﬁnite—differencing FD. This is carried out for several test problems on
ﬁve different platforms (processor7compiler) described in Table 4.1.

We include FD because this is often used in applications where an exact J is
not essential, such as in a Newton iteration to solve equations. FD can compete if it
computes J faster than does AD (sufﬁciently to outweigh a slower overall convergence).
This is almost never the case in the results we present, however.

We ﬁrst present in detail performance data on two test cases for eight LU—style
codes, preceded by results for other methods, cited from Forth et a]. [6]. Then, we
draw some observations which we further investigate using several examples most of
which were randomly generated.

In the tables 1" denotes (the code for) the Jacobian of f. For each technique,
we give (1) W(f’)/W(f), the ratio of the nominal number of ﬂoating—point operations
within the generated Jacobian code to those in the function code; (ii) the corre—
sponding ratios CPU(f’)/CPU(f) of CPU times for each platform. The W(f’) and
W(f) counts were obtained using a Perl script to count the number of operations in
executable statements. Each of *, + , - and / counts as one operation. They are
inaccurate estimates, since on the one hand each elementary function such as sqrt is
counted as one operation, as does raising to a power **. On the other they do not
take account of optimizations performed by the compiler: at the chosen optimization
levels, the compilers perform constant value propagation and evaluation [7, p. 32] to
avoid unnecessary operations, though the timings suggest this is not always well done.

For each test problem, a driver program was written to execute and time different
J acobian evaluation techniques. To check that the J acobians were calculated correctly,
we compared each to one produced by the hand—coded routine if available, and by the
ADIFOR or TAPENADE routine otherwise. Except for the FD results, discrepancies
were always at the roundoff level.

To improve consistency of timings, a number Nd (problem—dependent but typically
several hundred) of sets of input data was generated for a given problem using the

12 John D, Pryce and Emmanuel M, Tadjouddine

Fortran intrinsic routine random_nu.mber. The code for each technique was run Nd
times, once for each data set (if the same data were used repeatedly the cache might
not be ﬂushed, and a clever compiler might notice the re—use and “optimize out”
evaluations after the ﬁrst). The total time was divided by Nd to give the CPU time
estimate for that code. The process was repeated several times and results computed
for each repetition. The results for each repetition were nearly identical, showing that
the rankings are not due to vagaries of measurement in a multi—tasking system.

4.1. Initial Tests. The ﬁrst tests present data for the same compiler on three
machines and for the same machine with three compilers: and for a fairly large num—
ber of ways of generating Jacobian code. This illustrates that the performance varies
strongly between compilers and between machines, as well as between codes gener—
ated by the same overall algorithm with slightly different heuristics. We use the two
following examples. The distinction between SL and CL is explained on page 12.

ROE. The 5 X 10 dense Jacobian of the Roe ﬂux function [14], consisting of around
180 lines of Fortran 77 source. A SL differentiation of the input code yields a
sparse 67 X 72 matrix C with 198 nonzeros while its CL differentiation leads
to a sparse 213 X 218 matrix C with 344 nonzeros. Results are in Table 4.2.

FIC. The 32 X 32 Jacobian of the Flow in Channel problem from the MINPACK
test set [3]. This is sparse, with 244 out of a possible 1024 nonzeros. A
SL differentiation of the code yields a sparse 712 X 712 matrix C with 1276
nonzeros while its CL differentiation leads to a sparse 1360 X 1360 matrix C
with 1924 nonzeros. Results are in Table 4.3.

The FIC function uses loops heavily (nesting four deep) and this is reﬂected in
some of the Jacobian codes. To apply the VE/LU methods, the FIC function code was
loop—unrolled into straight—line form. We give ﬁgures for the code (marked “unrolled”)
that results from unrolling some of the Jacobian code produced by other tools. In
addition to the op—count and CPU—time ratios described above we give the Jacobian
object code ﬁle size in Kb (on AMD/g95). For FIC this shows, for instance, that
the hand—coded, TAPENADE and TAMC—R Jacobians, coded using loops, are short
but slow. The “unrolled” hand—coded version is the fastest, or close to it, on all the
platforms, but now of comparable length to the (loop—free) VE and LU codes. Even
for loop—free code, shorter is not always faster.

For each platform, the entry corresponding to the AD technique with the smallest
ratio of CPU times is highlighted in bold, and any entry with a ratio that is nearly
as small is underlined.

The ﬁrst rows are for the established tools ADIFOR, TAMC, TAPENADE, for a hand—
coded J acobian in the FIC case, and for approximation by one—sided ﬁnite differences.
Various versions of VE (Vertex Elimination) follow, produced by the ELIAD tool: and
ﬁnally the LU methods. For VE and LU the mnemonics F, R, Mark, VLR denote
ways of choosing the pivot order: forward order, reverse order, the order given by the
Markowitz algorithm, and Naumann’s [12] VLR variant on Markowitz. P stands for
pre—elimination: all vertices that have only one successor are eliminated ﬁrst (working
from the output end of the graph to the input) and then F, R, Mark or VLR is applied
to the remaining vertices.

SL (Statement Level) means that the computational graph to which methods
are applied is that of the original statements of the function code; CL (Code—list
Level) means it is the graph resulting from breaking each statement into constituent
elementary operations. For instance the statement c = sqrt(a*a + b*b) generates

AD Jacobians by compact LU 13

one vertex of the SL graph" but four of the CL graph. DFT denotes “depth—ﬁrst
traversal” of the Jacobian code [17], a post—processing phase devised by J.K. Reid,
where the statements are re—ordered aiming to place each assignment “v : ...” close
to the statements that use v.

For each platform we give the mean time for one f evaluation, CPU(f), at the
end of the table. For the Roe case, the variation in CPU(f’)/CPU(f) ratios are due
only slightly to differences in function time but mainly to differences in Jacobian
time. For FIC, the CPU(f) times for the three Dell compilers are in the wide ratio
of G95 : Absoft : Salford z 1 : 2 : 3, which distorts the ﬁgures 7 what look like very
good ratios for Salford are due to its having a comparatively slow function.

 

 

 

CPU(f’)/CPU(f) by platform
Technique WU”) fl obj AMD Intel Dell Dell Dell
W(f) Kb(AMD) G95 G95 G95 Salford Absoft
ADIFOR 15.95 17.0 17.01 15.76 9.37 6.96 13.29
TAPENADE 16.50 8.0 13.58 14.93 11.95 8.56 12.23
TAMC—F 21.18 10.1 22.06 25.26 13.58 12.96 15.94
TAMC—R 12.69 13.9 17.80 30.10 9.26 9.96 13.53
FD 12.14 7 12.83 15.28 14.53 10.52 12.41
VE—SL—F 8.89 30.9 45.19 44.68 26.16 7.87 13.23
VE—SL—R 7.32 25.6 42.27 43.21 24.79 7.30 11.23
VE—CL—F 12.85 27.5 41.35 42.08 25.32 16.48 13.77
VE—CL—R 9.50 26.1 42.26 45.33 24.47 13.52 11.29
VE—SL—P—F 7.85 27.3 6.77 9.56 4.47 4.61 5.41
VE—SL—P—R 6.78 22.7 5.78 9.21 ﬂ 4.22 5.00
VE—CL—P—F 8.35 26.8 6.36 10.27 4.32 5.57 5.24
VE—CL—P—R 7.28 23.2 & 9.09 & 5.09 &
VE—SL—P—F—DFT 7.85 26.0 7.20 10.57 5.00 5.26 5.59
VE—SL—P—R—DFT 6.78 21.6 6.22 8.08 4.63 4.96 4.65
VE—CL—P—F—DFT 8.35 25.4 6.42 10.98 4.58 8.61 5.24
VE—CL—P—R—DFT 7.28 21.0 5.72 8.56 4.58 8.39 4.41
VE—SL—P—Mark 7.35 25.8 6.44 11.27 4.47 4.39 5.00
VE—SL—P—VLR 6.60 22.9 5.92 8.68 4.21 4.13 4.35
VE—CL—P—Mark 7.86 26.1 6.08 11.27 4.53 5.17 5.12
VE—CL—P—VLR 7.11 22.9 ﬂ 9.21 4.26 4.96 g
VE—SL—P—Mark—DFT 7.35 23.9 6.37 8.97 5.16 4.87 5.18
VE—SL—P—VLR—DFT 6.60 20.7 5.99 7.79 4.53 4.70 4.59
VE—CL—P—Mark—DFT 7.86 23.5 6.30 9.26 4.53 8.35 5.06
VE—CL—P—VLR—DFT 7.11 19.8 5.34 7.85 3.95 8.22 4.12
SL—F—LU 4.25 23.8 7.30 6.85 5.11 4.35 5.71
SL—R—LU 3.66 21.2 6.13 6.85 4.37 w 5.24
SL—Mark—LU 3.56 21.1 6.28 6.67 4.37 4.04 5.06
SL—VLR—LU 3.43 19.1 5.80 5.61 4.53 3.74 4.35
SL—P—F—LU 3.78 22.0 6.49 6.55 4.89 4.17 5.35
SL—P—R—LU 3.44 19.9 5.79 M 4.32 3.74 4.88
SL—P—Mark—LU 3.54 20.9 6.24 7.03 4.47 4.00 5.59
SL—P—VLR—LU 3.36 18.7 5.34 6.02 ﬂ 3.74 g
Mean f evaluation time (us) .19 .25 .48 .53 .43
TABLE 42

 

 

Tests on Roe flu$.

*The local derivatives of a statement are computed by local reverse mode, as in ADIFOR,

14 John D, Pryce and Emmanuel M, Tadjouddine

 

 

 

CPU(f’)/CPU(f) by platform
Technique W(f’) f’ obj AMD 111461 Dell Dell Dell
W(f) Kb(AMD) G95 G95 G95 Salford Absoft
Hand—Coded 1.91 1.4 6.29 6.72 5.92 2.83 4.22
Hand—Coded unrolled 1.91 31.0 & 5.47 2.65 1.51 3.26
ADIFOR unrolled 34.42 51.5 95.07 107.67 94.90 19.43 67.95
TAPENADE 35.99 2.1 106.36 102.13 94.49 31.51 68.98
TAMC—F unrolled 35.99 50.4 88.79 98.01 84.49 20.38 68.98
TAMC—R 44.64 2.5 106.47 110.21 90.75 33.59 73.19
FD unrolled 33.73 7 133.93 111.40 99.80 47.74 76.04
VE—SL—F 3.49 34.3 2.76 4.77 2.99 4.15 3.26
VE—SL—R 2.25 34.5 2.70 4.61 2.92 2.83 3.29
VE—CL—F 4.44 38.0 3.21 10.01 4.49 6.60 16.20
VE—CL—R 2.75 33.2 2.66 5.65 3.27 4.34 2.81
VE—SL—P—F 2.25 34.5 2.67 4.62 2.92 2.83 3.07
VE—SL—P—R 2.25 34.5 2.65 4.62 2.92 2.83 3.10
VE—CL—P—F 2.75 33.2 2.65 5.90 3.27 4.53 2.78
VE—CL—P—R 2.75 33.2 2.66 5.80 3.20 4.34 2.81
VE—SL—P—F—DFT 2.25 28.8 3.12 3.52 ﬂ 3.96 2.36
VE—SL—P—R—DFT 2.25 28.8 3.12 m ﬂ 4.15 2.40
VE—CL—P—F—DFT 2.75 27.4 2.89 4.66 ﬂ 7.92 &
VE—CL—P—R—DFT 2.75 27.4 2.91 4.62 ﬂ 7.74 1.76
VE—SL—P—Mark 2.25 34.5 2.65 4.60 2.92 2.83 3.10
VE—SL—P—VLR 2.25 34.5 2.65 4.59 2.92 2.83 3.16
VE—CL—P—Mark 2.75 33.2 2.66 5.87 3.20 4.34 2.81
VE—CL—P—VLR 2.75 33.2 2.66 5.88 3.27 4.15 2.72
VE—SL—P—Mark—DFT 2.25 28.8 3.12 ﬂ ﬂ 4.15 2.40
VE—SL—P—VLR—DFT 2.25 28.8 3.11 m 2.65 4.15 2.36
VE—CL—P—Mark—DFT 2.75 27.4 2.91 4.67 ﬂ 7.74 &
VE—CL—P—VLR—DFT 2.75 27.4 2.89 4.69 2.79 7.74 &
SL—F—LU 1.72 30.8 H 5.57 2.99 M 3.10
SL—R—LU 1.72 30.8 & 5.73 2.99 M 3.07
SL—Mark—LU 1.72 30.9 g 5.45 2.99 M 3.10
SL—VLR—LU 1.72 30.8 H 5.48 2.99 M 3.13
SL—P—F—LU 1.72 30.8 H 5.64 2.92 m 3.07
SL—P—R—LU 1.72 30.8 2.44 5.61 2.92 M 3.10
SL—P—Mark—LU 1.72 30.8 ﬂ 5.53 2.99 M 3.13
SL—P—VLR—LU 1.72 30.8 ﬂ 5.50 2.99 M 3.10
Mean f evaluation time (us) 0.61 0.81 0.92 3.3 2.0

 

 

TABLE 43
Tests on FIC problem.

The difference between platforms is striking. Those between different compilers
on one machine suggests they use very different methods to optimize code]. Note
for instance on the FIC problem, comparing the Dell results, VE—CL—P—R—DFT is
4.4 times faster with Absoft than Salford, while ADIFOR—unrolled is 3.5 times faster
with Salford than Absoft, a “discrepancy ratio” of 4.4 X 3.5 : 15.4. This is the most
extreme of many similar discrepancies.

1LAbsoft support told us optimization level —02 was suitable: higher levels made no appreciable
speed difference to this kind of code while increasing the length of the binary, For G95, —03 seemed
best for similar reasons, Salford only has one optimization level, i.e. it is either “off” or “on”,

AD Jacobians by compact LU 15

We believe the variation between different machines with the same compiler is
mainly due to how they manage the memory hierarchy. The processors we used
have a cache—based memory hierarchy with the time to load data into the arithmetic
registers increasing from the level—1 cache through level—2 cache to the main memory.
Arithmetic registers are limited, typically 32 or 64. Optimizing compilers usually
seek to maximize performance by minimizing redundant calculations as well as the
memory traﬂic between registers and cache. Stalls arise when an instruction calls
for a value not yet loaded into registers causing the processor to wait. Some of the
processors such as the AMD or Intel in our study feature out—of—order e$ecution, in
which the processor maintains a queue of arithmetic operations so that if the one at
the head of the queue stalls, it can switch to an operation in the queue that is able to
execute. More details on such issues may be found in [7]. We do not at present have
any way to predict the performance of one of our VE / LU techniques, though some
work in this direction is presented in [15]. This included study of assembler code and
register usage. Because we were unable, so far, to ﬁnd clear patterns emerging we do
not present such data here.

For both Roe and FIC problems, on all ﬁve platforms, all the VE/LU—style meth—
ods vastly outperform the standard tools and can even outperform “hand—coded un—
rolled”. For the Roe problem, one of the LU methods gives best or nearly—best perfor—
mance on all platforms. For FIC, it is remarkable that 244 entries of f’ are computed
in such a small multiple of the time to compute only f. This must be because FIC’s f
is in fact a linear function. For FIC, the LU methods are strikingly better than other
VE methods on the Dell/Salford platform (hand—coded unrolled is even better), signif—
icantly worse than VE methods post—processed by DFT on Intel/G95, and comparable
with other VE methods on the other platforms. We have not tried post—processing
LU—style code by DFT, but the results suggest it could be useful.

What is clear is that the LU results are far less sensitive to the pivot heuristic
used than are the other VE results. In all our experiments, on all ﬁve platforms, LU
methods7 performance varied only modestly over these 8 pivot heuristics. Assuming
this is true more generally it is a signiﬁcant reason to prefer LU to other VE methods.

4.2. Further Tests. Subsection 2.4 mentioned that for the widely used AD tool
TAPENADE the simplest way to generate Jacobian code is vector mode, which does not
exploit sparsity unless one uses separate methods such as Jacobian compression [8,
Ch. 7]. One should thus expect the LU approach to outperform it, at least provided
the problem is not so large that the LU code is dominated by data transfer. To
investigate this further, the following examples compare it to TAPENADE and to ﬁnite—
differencing, on a number of problems of increasing size till the LU approach clearly
runs into diﬂiculties.

First we use the problems:

PLEI. The 28 X 28 Jacobian of the Pleiades problem from the IVP (Initial Value
Problem) test set [10]. This is sparse, with 210 out of a possible 784 nonzeros.
Using for example the CL differentiation, the corresponding C matrix is of
size 630 X 630 with 994 nonzeros (sparsity 0.25%).

RC1. The 300 X 300 Jacobian of a function randomly generated by our MATLAB tool
by setting n:300,m:300 and p:1450. This has 4635 nonzero entries out of
a possible 90, 000. This is already in CL form and its corresponding C matrix
is of size 1750 X 1750 with 994 nonzeros.

RC2. The 122 X 255 Jacobian of a function randomly generated by our MATLAB tool
by setting n:255,m:122 and p:2650. This has 7366 nonzero entries out of

16 John D, Pryce and Emmanuel M, Tadjouddine

 

 

 

 

 

 

 

 

 

 

CPU(f’)/CPU(f) by platform
Technique W(f’) AMD 111461 Dell Dell Dell
W(f) G95 G95 G95 Salford Absoft
PLEI problem
HAND—CODED 1.25 1.40 1.57 2.29 1.50 1.36
TAPENADE 12.3 8.55 10.2 19.0 8.87 7.82
FD 5.29 29.5 29.9 31.6 30.0 26.3
LU best 2.83 2.23 2.78 4.92 2.70 1.85
achieved by P—Mark R P—VLR F P—VLR
RC1 problem, (n, m,p) = (300, 300, 1450) with 4635 nonzeros
TAPENADE 331. 7760. 3980. 2810. 2240. 589.
FD 301. 483. 517. 460. 362. 272.
LU best 4.12 488. 332. 426. 341. 147.
achieved by P—VLR VLR P—Mark P—Mark P—VLR
RC2 problem, (n,m,p) = (255, 122, 2650) with 7366 nonzeros
TAPENADE 636. 7280. 4310. 3440. 3990. 450.
FD 256. 284. 203. 298. 94.5 297.
LU best 6.86 111. 51. 52.8 10.0 51.2
achieved by P—Mark VLR F F F
RC3 problem, (n,m,p) = (222, 300,3250) with 14,569 nonzeros
TAPENADE 554. 6550. 6310. 7 1240. 1030.
FD 223. 293. 271. 7 259. 190.
LU best 7.68 326. 310. 7 99.2 104.
achieved by VLR VLR P—Mark P—R
A runnable program for R03 could not be created on Dell/695.

 

TABLE 44
Tests on PLEI and random problems. For reasons of space, the names of the LU methods are
abbreviated, e.g. P- VLR has the meaning of SL-P— VLR-LU in the previous tables.

a possible 31, 110. This is already in CL form and its corresponding C matrix
is of size 2672 X 2905 with 5544 nonzeros.

RC3. The 300 X 222 Jacobian of a function randomly generated by our MATLAB tool
by setting n:222,m:300 and p:3250. This has 14,569 nonzero entries out
of a possible 66,600. This is already in CL form and its corresponding C
matrix is of size 3550 X 3472 with 7099 nonzeros.

For completeness, Table 4.5 gives nominal ﬂop—counts, lines of code and function—
time statistics for the above test cases (but not those used for Figure 4.1).

The TAPENADE vector mode treats the jth column of the Jacobian J as the direc—
tional derivative (ej -V) f where ej is the jth unit vector, and computes these in a loop
one column at a time. The unit vectors are the columns of the n X n identity matrix,
which is input to the process as a “seed matrix”. (For a general n X q seed matrix
S, the output is JS: this can be exploited, e.g., by Jacobian compression methods.)
Vector mode also uses optimization techniques such as common subexpression elimi—
nation and some code motion. However, it may perform useless calculations such as
multiplying by or adding to zero.

Results are shown in Table 4.4. For the random functions, the ﬂop—counts of LU—
style codes are dramatically less than those of the corresponding TAPENADE code.
This is because these represent sparse computations and the LU approach can exploit
the sparsity. The LU—style codes run roughly 4 to 20 times as fast as the TAPENADE
ones. However, FD (though of course inaccurate) provides run times similar to and

AD Jacobians by compact LU 17

FIG, 41, Performance on random functions of sizes (n,m,p) : (15i71,15i76,113i) for
i : 1,2, , , , ,20 (largest size in ﬁgure is (301, 306, 2260)). The number of statements (each an
elementary operation) in f is m 7 p, the size of the Jacobian is n X m, and FD calls the function
71 7 1 times to compute the Jacobian. The sparsity of the Jacobian decreases fairly uniformly from
58% for the smallest to 10.3% for the largest.

occasionally faster than the LU—based approach for the random functions.

To investigate this observation, we applied these techniques on more codes gen—
erated by our random function tool. Figure 4.1 shows the behaviour with increasing
size, graphically. Random functions of sizes (n,m,p) : (15i 7 1,15i 7 6,113i) for
i : 1,2, . .. were generated. The ﬁgure shows the CPU(f’)/CPU(f) ratios of TAPE—
NADE, FD and the best and worst of the eight LU methods. In this test, clearly the
latter are quite insensitive to the choice of heuristic; as problem size increases TAPE—
NADE is progressively slower than them, while FD is catching up. The anomalous
peak and trough in the TAPENADE results with Salford are genuine: repeated runs
gave almost identical results.

It seems the key factor is not the length of code, but the size, or probably the
number of nonzero entries, of the J acobian. On the AMD and Intel platforms, above
a certain size, FD is faster than the LU techniques 7 probably because the Jacobian
entries cannot ﬁt into the cache and data transfer is dominating the computation.
The level 1 cache holds about 4,000 double—precision values on the AMD and Intel
machines, about 8,000 on the Dell.

How important is the pivot order? In the Roe example, VE with simple Forward
and Reverse order, both at statement and at code—list level, were far worse than
the other VE/LU—style methods on all the platforms; but combining them with pre—
elimination gave orders competitive with the other methods. In the FIC case the
different orders for VE and LU, with and without DFT post—processing, were far more
similar to each other with the exception of “outliers” such as the bad time for VE—CL—
F on Intel/G95, and the fact that VE—CL—P—R—DFT was the best on Dell/Absoft but
nearly the worst VE/LU—style method on Dell/Salford. For the random functions,
we only have different LU—style methods to compare: again, the pivot order does not
make a dramatic difference in the tests shown.

We have also tried randomly generated pivot orders. They always gave perfor—
mance far worse than the ones presented in this paper. Thus, the order matters, but
at present we have no way to tell which order will be best for a given platform.

 

 

 

W(f) Function CPU time(ns)
Problem (ﬂops) 1.0.0. AMD Intel Dell Dell Dell
G95 G95 G95 SalfordAbsoft
Roe 222 139 .19 .25 .48 .53 .43
FIG 1266 759 .61 .81 .92 3.3 2.0
PLEI 735 238 .70 .86 13. 3.9 9.3
RC1 1750 1750 3.0 5.2 7.5 9.0 10.
RC2 2772 2772 4.8 7.5 13. 50. 13.
RC3 3550 3550 6.1 8.8 7 21. 20.

TABLE 45
Nominal floating-point operations counts W(f), lines of code (Lac) and CPU times for test
problem functions

18 John D, Pryce and Emmanuel M, Tadjouddine

5. Conclusions. The vertex elimination (VE) approach to calculating the Ja—
cobian J of a vector function f is usually described in terms of the computational
graph. Earlier work, e.g. [6] using the ELIAD tool, showed the potential speedup of
VE—style methods compared with other AD tools for generating Jacobian code. We
have shown how VE is equivalent to a (partial) Gaussian elimination or LU factor—
ization of a sparse matrix. This View offers different ways to sequence the operations
that are not obvious in the graph approach.

For f with straight—line code, we have used source transformation to generate
straight—line J acobian code using an order of operations derived from Crout compact
LU factorization. For the previous VE work with ELIAD, elimination pivot orders
had been generated by Markowitz—style and other heuristics. For each such order a
corresponding LU code was produced. On all platforms tried, the best LU version
was always nearly as fast as the best VE version and often signiﬁcantly faster; and
the LU codes are less sensitive to the choice of pivot heuristic. Both VE and LU are
usually many times faster than codes generated by tools such as ADIFOR, TAMC and
the more recent TAPENADE. For the FIC problem an LU version was signiﬁcantly
faster than a hand coded Jacobian on three out of the ﬁve platforms used.

These initial problems were small enough for all the values in the computation
to ﬁt in the cache of current machines. The large performance variation between
platforms, and between different VE/LU methods on the same platform, is there—
fore mainly due to differences in pipeline, register and cache scheduling on different
systems. We believe a deeper understanding of these issues will lead to signiﬁcant
further speedups by re—ordering the operations involved in factorization, and that for
problems of this size, data transfer from and to main memory is less important.

Cache usage will be improved by minimizing the active set, of computed values
at any stage that need to be used in the future. The DFT approach (Subsection 4.1)
tried to do this with uncertain success; many sparse matrix methods as in Duff et al.
[5, Chapters 7710] describe approaches whose matrix access patterns are in a sense
intermediate between Gaussian elimination and compact LU factorization, and might
be used to “tune” the active set.

Implementing AD directly within the compiler, as the CompAD project of Nau—
mann et al. [13] is doing with the NAG compiler, is a new development that is likely
to lead to just such improvements in low—level optimization of AD.

The pivot order matters, but at present we have no way to tell which order will be
best for a given platform. To get near—best performance of VE/LU—style Jacobians,
one may follow the methods of the ATLAS project [2], to generate and run alternative
codes automatically and choose the one that runs fastest on a given platform.

We then examined “larger” functions, using randomly generated functions of in—
creasing size, to see what happens when there are too many values to ﬁt into cache,
with the LU codes eventually becoming slower than ﬁnite differencing. We believe the
number of non—zeros in the ﬁnal J acobian has more effect than the number of inter—
mediate values in the computation. For these larger problems, main memory traﬂic
is obviously key and the approach of storing matrix entries as simple variables should
be replaced by one that gives control over memory arrangement and data structure.
Standard sparse factorization techniques [5] then become far more relevant.

In summary, our results indicate Jacobian code based on LU factorization should
be used in such cases as:

o a heavily used function of moderate length that can be coded up without
loops, such as the Roe ﬂux and similar ﬂux functions whose Jacobians are

AD Jacobians by compact LU 19

needed repeatedly for assembling global Jacobians in CFD; or

o a basic block within a larger section of code, in cases where a compiler can de—
termine, or be told, that this basic block is crucial to the overall performance
of the program.

Acknowledgements. We wish to thank the anonymous referees, as well as

Shaun Forth and John Reid, for their careful reading of the text and constructive
comments.

[1]

[2

l3]

l4]

l5]

l6]

l7]
l8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

REFERENCES

P, R, AMESTOY, X, S, LI AND E, G, NG, Diagonal Markowitz scheme with local symmetrization,
Preprint, Lawrence Berkeley National Laboratory, Berkeley, CA 93720, USA (2005).
JACK DONGARRA, ANTOINE PETITET, R. CLINT WHALEY et al., ATLAS (Automatically Tuned

Linear Algebra Software) project. See http://math-atlas.sourceforge.net

B. M, AVERICK, R. G, CARTER, J, J, MORE, AND G,-L. XUE, The MINPACK—2 test prob-
lem collection, Preprint MCS7P153{)692, ANL/MCS7TM7150, Rev, 1, Mathematics and
Computer Science Division, Argonne National Laboratory, Argonne, Ill, (1992), See
ftp: //info . mcs . anl . gov/pub/MINPACK-2/tprobs/P153 . ps . Z

R, CYTRON, J, FERRANTE, B, K, ROSEN, M, N, WEGMAN, AND F, K, ZADECK, Efﬁciently com-
puting static single assignmentform and the control dependence graph, ACM Transactions
on Programming Languages and Systems, 13 (1991), pp, 4517490,

I, S, DUFF, A, M, ERISMAN AND J, K, REID, Direct Methods for Sparse Matrices, Oxford
Science Publications (1986),

S, A. FORTH, M, TADJOUDDINE, J, D. PRYCE, AND J, K. REID, Jacobian code generated by
source transformation and verte$ elimination can be as efﬁcient as hand-coding, ACM
Transactions on Mathematical Software, 30 (2004), pp, 2667299.

S, GOEDECKER AND A, HOISIE, Performance Optimization of Numerically Intensive Codes,
SIAM Philadelphia (2001),

A, GRIEWANK, Evaluating Derivatives: Principles and Techniques of Algorithmic Diﬁerentia-
tion, no, 19 in Frontiers in Appl, Math,, SIAM, Philadelphia, Penn, (2000),

A, GRIEWANK AND S, REESE, On the calculation of Jacobian matrices by the Markowitz rule,
in Automatic Differentiation of Algorithms: Theory, Implementation, and Application,
A, Griewank and G, F, Corliss, eds,, SIAM, Philadelphia, Penn, (1991), pp, 1267135,

F, MAZZIA AND F, IAVERNARO, Test set for initial value problem solvers, technical paper,
Department of Mathematics, University of Bari (2003), See www.dm.unibs. it/"testset

U, NAUMANN, Efﬁcient Calculation of Jacobian Matrices by Optimized Application of the Chain
Rule to Computational Graphs, PhD thesis, Technical University of Dresden (December
1999).

, Elimination techniques for cheap Jacobians, in Automatic Differentiation: From Simu—
lation to Optimization, G, Corliss, C, Faure, A, Griewank, L, Hascoet, and U, Naumann,
eds,, Computer and Information Science, Springer, New York (2001), ch, 29, pp, 2417246,

U. NAUMANN, B. CHRISTIANSON, J, RIEHME AND DMITRIJ GENDLER, Differentiation Enabled
Fortran Compiler Technology, University of Aachen, Germany; University of Hertford—
shire, UK; Numerical Algorithms Group Ltd, UK, See http://www.nag.co.uk/nagware/
research/ad_overview.asp

P, L, ROE, Approrimate Riemann solvers, parameter vectors, and diﬁerence schemes, Journal
of Computational Physics, 43 (1981), pp, 3577372.

M, TADJOUDDINE, F, BODMAN, J, D, PRYCE, AND S, A, FORTH, Improving the performance of
the verte$ elimination algorithm for derivative calculation, in AD2004: Proceedings of the
4th International Conference on Automatic Differentiation, M, Bucker et al., ed,, vol, 50 of
Lecture Notes in Computational Science and Engineering, Springer (2005), pp, 1117120,

M, TADJOUDDINE, S, A, FORTH, AND J, D, PRYCE, Hierarchical automatic diﬁerentiation
by verte$ elimination and source transformation, in ICCSA (2), V, Kumar et al., ed,,
vol, 2668 of LNCS, Springer (2003), pp, 1157124,

M, TADJOUDDINE, S, A, FORTH, J, D, PRYCE, AND J, K, REID, Performance issues for verte$
elimination methods in computing Jacobians using automatic diﬁerentiation., in ICCS (2),
P, M, A, Sloot et al., ed,, vol, 2330 of LNCS, Springer (2002), pp. 107771086.

 

